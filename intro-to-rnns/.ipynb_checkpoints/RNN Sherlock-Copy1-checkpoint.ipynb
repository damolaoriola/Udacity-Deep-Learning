{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3864202"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting features from text\n",
    "\n",
    "with open('cano.txt', 'r') as f:\n",
    "    book = f.read()\n",
    "\n",
    "create_set = sorted(set(book))\n",
    "\n",
    "dict_int = {word: inte for inte, word in enumerate(create_set)}\n",
    "\n",
    "\n",
    "#create array of entire book\n",
    "encoded_book = np.array([dict_int[word] for word in book], dtype = np.int32)\n",
    "\n",
    "len(encoded_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          CHAPTER I\n",
      "          Mr. Sherlock Holmes\n",
      "\n",
      "\n",
      "     In the year 1878 I took my degree of Docto\n",
      "[ 0  1  1  1  1  1  1  1  1  1  1 28 33 26 41 45 30 43  1 34  0  1  1  1\n",
      "  1  1  1  1  1  1  1 38 72 11  1 44 62 59 72 66 69 57 65  1 33 69 66 67\n",
      " 59 73  0  0  0  1  1  1  1  1 34 68  1 74 62 59  1 79 59 55 72  1 14 21\n",
      " 20 21  1 34  1 74 69 69 65  1 67 79  1 58 59 61 72 59 59  1 69 60  1 29\n",
      " 69 57 74 69]\n"
     ]
    }
   ],
   "source": [
    "len(create_set)\n",
    "\n",
    "print(book[:100])\n",
    "\n",
    "'\\n' '\\n'\n",
    "\n",
    "print(encoded_book[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for gettin batches\n",
    "\n",
    "def get_batches(arr, batch_size, num_steps):\n",
    "\n",
    "    character_per_batch = batch_size * num_steps\n",
    "\n",
    "    num_batches = len(arr)//character_per_batch\n",
    "\n",
    "\n",
    "    #keep only enough to make full batches\n",
    "\n",
    "    arr = arr[: (character_per_batch * num_batches)]\n",
    "\n",
    "    #reshape into batch_size\n",
    "\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "\n",
    "\n",
    "    #split into x & y\n",
    "\n",
    "    for step in range(0, arr.shape[1], num_steps):\n",
    "\n",
    "        x = arr[:, step : step + num_steps]\n",
    "\n",
    "        y_temp = arr[:, (step + 1): (step+1) + num_steps]\n",
    "    \n",
    "        y = np.zeros(x.shape, dtype = x.dtype)\n",
    "    \n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "    \n",
    "        yield x, y\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  1  1  1  1  1  1  1  1  1 28]\n",
      " [74 73 69 68 11  1 48 59  1 73 62 55]\n",
      " [69 73 74  1 57 69 67 59  1 74 69  1]\n",
      " [ 9  1 55 68 58  1 62 69 77  1 62 59]\n",
      " [63 68  1 74 69 10 67 69 72 72 69 77]\n",
      " [72 63 68 61  1 63 68  1 74 62 59  1]\n",
      " [58  1 69 60  1 55  1 67 55 72 72 63]\n",
      " [60 63 57 63 55 66  1 59 78 70 69 68]\n",
      " [59 55 72 73 11  0  0  1  1  1  1  1]\n",
      " [ 1 56 59  1 74 72 75 73 74 59 58 11]]\n",
      "\n",
      " [[ 1  1  1  1  1  1  1  1  1  1 28 33]\n",
      " [73 69 68 11  1 48 59  1 73 62 55 66]\n",
      " [73 74  1 57 69 67 59  1 74 69  1 56]\n",
      " [ 1 55 68 58  1 62 69 77  1 62 59  1]\n",
      " [68  1 74 69 10 67 69 72 72 69 77  9]\n",
      " [63 68 61  1 63 68  1 74 62 59  1 61]\n",
      " [ 1 69 60  1 55  1 67 55 72 72 63 59]\n",
      " [63 57 63 55 66  1 59 78 70 69 68 59]\n",
      " [55 72 73 11  0  0  1  1  1  1  1  3]\n",
      " [56 59  1 74 72 75 73 74 59 58 11  1]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded_book, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print(x[:12, :12])\n",
    "print('\\n',y[:12, :12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input, output & keep_prob\n",
    "\n",
    "def tensor_variables(batch_size, num_steps):\n",
    "\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LSTM Cell\n",
    "\n",
    "def LSTM(lstm_size, batch_size, keep_prob, num_layers):\n",
    "\n",
    "    #Build lstm cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "    \n",
    "        Lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "        drop = tf.contrib.rnn.DropoutWrapper(Lstm, output_keep_prob = keep_prob)\n",
    "        \n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    multi_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    initial_state = multi_lstm.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return multi_lstm, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating predictions from our network\n",
    "\n",
    "def logits(lstm_output, lstm_size, outclasses_size ):\n",
    "\n",
    "\n",
    "    lstm_batch_list = tf.concat(lstm_output, axis =1)\n",
    "    \n",
    "    lstm_output = tf.reshape(lstm_batch_list, [-1, lstm_size])\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope ('softmax'):\n",
    "        \n",
    "        softmax_w = tf.get_variable('softmax_w', [lstm_size, outclasses_size], initializer = tf.contrib.layers.xavier_initializer(seed =1))\n",
    "        \n",
    "        softmax_b = tf.get_variable('softmax_b', [outclasses_size], initializer = tf.zeros_initializer())\n",
    "        \n",
    "    \n",
    "    logits = tf.add(tf.matmul(lstm_output, softmax_w), softmax_b)\n",
    "    \n",
    "    \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss\n",
    "\n",
    "def loss_hot(logits, targets, num_classes):\n",
    "    \n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    \n",
    "    y = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(learning_rate, grad_clip, loss):\n",
    "\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PROCESS\n",
    "\n",
    "class SherlockAI:\n",
    "    \n",
    "    def __init__(self, num_classes, lstmsize = 128, learning_rate = 0.01, \n",
    "                 batch_size = 64, num_steps = 50, num_layers = 2, \n",
    "                  sampling = False, grad_clip=5):\n",
    "    \n",
    "    \n",
    "        if sampling == True:\n",
    "        \n",
    "            batch_size, num_steps = 1, 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #Build the input placeholders\n",
    "        self.inputs, self.targets, self.keep_prob = tensor_variables(batch_size, num_steps)\n",
    "        \n",
    "        #Build LSTM Cell architecture                                                             \n",
    "        lstm, self.initial_state =  LSTM(lstmsize, batch_size, keep_prob, num_layers)\n",
    "        \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        #Run inputs through LSTM Cell\n",
    "        \n",
    "        outputs, final_state = tf.nn.dynamic_rnn(lstm, x_one_hot, initial_state = self.initial_state)\n",
    "        \n",
    "        self.final_state = final_state\n",
    "        \n",
    "        #Predictions using lstm run\n",
    "        \n",
    "        self.logits, self.predictions =  logits(outputs, lstmsize, num_classes)\n",
    "        \n",
    "        #loss function & optimizer\n",
    "        \n",
    "        self.loss = loss_hot(self.logits, self.targets, num_classes)\n",
    "        self.optimizer = build_optimizer(learning_rate, grad_clip, self.loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstmsize = 540       # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.1        # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3776321\n",
      "epochs: 1/5.....  training_step: 50....  training_loss: 2.970223.......  2.4896 sec/batch \n",
      "epochs: 1/5.....  training_step: 100....  training_loss: 2.887045.......  2.6625 sec/batch \n",
      "epochs: 1/5.....  training_step: 150....  training_loss: 2.761403.......  2.2754 sec/batch \n",
      "epochs: 1/5.....  training_step: 200....  training_loss: 2.693883.......  2.0575 sec/batch \n",
      "epochs: 1/5.....  training_step: 250....  training_loss: 2.299656.......  2.0402 sec/batch \n",
      "epochs: 1/5.....  training_step: 300....  training_loss: 2.223397.......  2.0334 sec/batch \n",
      "epochs: 1/5.....  training_step: 350....  training_loss: 2.124370.......  2.0560 sec/batch \n",
      "epochs: 1/5.....  training_step: 400....  training_loss: 2.135356.......  2.0914 sec/batch \n",
      "epochs: 1/5.....  training_step: 450....  training_loss: 2.151492.......  2.0522 sec/batch \n",
      "epochs: 1/5.....  training_step: 500....  training_loss: 2.026647.......  2.0251 sec/batch \n",
      "epochs: 1/5.....  training_step: 550....  training_loss: 1.941062.......  2.0577 sec/batch \n",
      "epochs: 1/5.....  training_step: 600....  training_loss: 1.939425.......  2.0172 sec/batch \n",
      "epochs: 1/5.....  training_step: 650....  training_loss: 1.906621.......  2.0409 sec/batch \n",
      "epochs: 1/5.....  training_step: 700....  training_loss: 1.958073.......  2.0325 sec/batch \n",
      "epochs: 1/5.....  training_step: 750....  training_loss: 2.024940.......  2.0060 sec/batch \n",
      "epochs: 1/5.....  training_step: 800....  training_loss: 2.075301.......  2.0452 sec/batch \n",
      "epochs: 1/5.....  training_step: 850....  training_loss: 1.685113.......  2.0673 sec/batch \n",
      "epochs: 1/5.....  training_step: 900....  training_loss: 1.628809.......  2.0383 sec/batch \n",
      "epochs: 1/5.....  training_step: 950....  training_loss: 1.737653.......  2.0463 sec/batch \n",
      "epochs: 1/5.....  training_step: 1000....  training_loss: 1.656468.......  2.0364 sec/batch \n",
      "epochs: 1/5.....  training_step: 1050....  training_loss: 1.617780.......  2.1815 sec/batch \n",
      "epochs: 1/5.....  training_step: 1100....  training_loss: 1.535981.......  2.0530 sec/batch \n",
      "epochs: 1/5.....  training_step: 1150....  training_loss: 1.674955.......  2.0453 sec/batch \n",
      "epochs: 1/5.....  training_step: 1200....  training_loss: 1.599832.......  2.2091 sec/batch \n",
      "epochs: 1/5.....  training_step: 1250....  training_loss: 1.574227.......  2.0311 sec/batch \n",
      "epochs: 1/5.....  training_step: 1300....  training_loss: 1.519133.......  2.1370 sec/batch \n",
      "epochs: 1/5.....  training_step: 1350....  training_loss: 1.465077.......  2.0513 sec/batch \n",
      "epochs: 1/5.....  training_step: 1400....  training_loss: 1.551683.......  2.0445 sec/batch \n",
      "epochs: 1/5.....  training_step: 1450....  training_loss: 1.439748.......  2.0486 sec/batch \n",
      "epochs: 1/5.....  training_step: 1500....  training_loss: 1.451172.......  2.0628 sec/batch \n",
      "epochs: 1/5.....  training_step: 1550....  training_loss: 1.366376.......  2.1864 sec/batch \n",
      "epochs: 1/5.....  training_step: 1600....  training_loss: 1.452893.......  2.1261 sec/batch \n",
      "epochs: 1/5.....  training_step: 1650....  training_loss: 1.380959.......  2.0594 sec/batch \n",
      "epochs: 1/5.....  training_step: 1700....  training_loss: 1.565300.......  2.0344 sec/batch \n",
      "epochs: 1/5.....  training_step: 1750....  training_loss: 1.426171.......  2.0328 sec/batch \n",
      "epochs: 1/5.....  training_step: 1800....  training_loss: 1.381874.......  2.0280 sec/batch \n",
      "epochs: 1/5.....  training_step: 1850....  training_loss: 1.413955.......  2.0284 sec/batch \n",
      "epochs: 1/5.....  training_step: 1900....  training_loss: 1.387352.......  2.0157 sec/batch \n",
      "epochs: 1/5.....  training_step: 1950....  training_loss: 1.437382.......  2.0810 sec/batch \n",
      "epochs: 1/5.....  training_step: 2000....  training_loss: 1.444367.......  2.0149 sec/batch \n",
      "epochs: 1/5.....  training_step: 2050....  training_loss: 1.314560.......  2.0188 sec/batch \n",
      "epochs: 1/5.....  training_step: 2100....  training_loss: 1.390643.......  2.1883 sec/batch \n",
      "epochs: 1/5.....  training_step: 2150....  training_loss: 1.314784.......  2.2116 sec/batch \n",
      "epochs: 1/5.....  training_step: 2200....  training_loss: 1.372040.......  2.1043 sec/batch \n",
      "epochs: 1/5.....  training_step: 2250....  training_loss: 1.267593.......  2.0626 sec/batch \n",
      "epochs: 1/5.....  training_step: 2300....  training_loss: 1.214668.......  2.0369 sec/batch \n",
      "epochs: 1/5.....  training_step: 2350....  training_loss: 1.280763.......  2.0023 sec/batch \n",
      "epochs: 1/5.....  training_step: 2400....  training_loss: 1.302636.......  2.0548 sec/batch \n",
      "3776321\n",
      "epochs: 2/5.....  training_step: 2450....  training_loss: 1.279456.......  2.1587 sec/batch \n",
      "epochs: 2/5.....  training_step: 2500....  training_loss: 1.319448.......  2.0046 sec/batch \n",
      "epochs: 2/5.....  training_step: 2550....  training_loss: 1.327425.......  1.9978 sec/batch \n",
      "epochs: 2/5.....  training_step: 2600....  training_loss: 1.308000.......  2.0439 sec/batch \n",
      "epochs: 2/5.....  training_step: 2650....  training_loss: 1.329581.......  1.9928 sec/batch \n",
      "epochs: 2/5.....  training_step: 2700....  training_loss: 1.366505.......  2.0414 sec/batch \n",
      "epochs: 2/5.....  training_step: 2750....  training_loss: 1.331009.......  2.0055 sec/batch \n",
      "epochs: 2/5.....  training_step: 2800....  training_loss: 1.208156.......  2.0162 sec/batch \n",
      "epochs: 2/5.....  training_step: 2850....  training_loss: 1.245066.......  2.8182 sec/batch \n",
      "epochs: 2/5.....  training_step: 2900....  training_loss: 1.204098.......  2.6805 sec/batch \n",
      "epochs: 2/5.....  training_step: 2950....  training_loss: 1.211148.......  2.3457 sec/batch \n",
      "epochs: 2/5.....  training_step: 3000....  training_loss: 1.170260.......  2.1867 sec/batch \n",
      "epochs: 2/5.....  training_step: 3050....  training_loss: 1.276866.......  2.1398 sec/batch \n",
      "epochs: 2/5.....  training_step: 3100....  training_loss: 1.164211.......  2.0318 sec/batch \n",
      "epochs: 2/5.....  training_step: 3150....  training_loss: 1.149800.......  3.1116 sec/batch \n",
      "epochs: 2/5.....  training_step: 3200....  training_loss: 1.199915.......  2.1177 sec/batch \n",
      "epochs: 2/5.....  training_step: 3250....  training_loss: 1.205133.......  2.1051 sec/batch \n",
      "epochs: 2/5.....  training_step: 3300....  training_loss: 1.263365.......  2.1210 sec/batch \n",
      "epochs: 2/5.....  training_step: 3350....  training_loss: 1.175952.......  2.0821 sec/batch \n"
     ]
    }
   ],
   "source": [
    "#Training phase\n",
    "\n",
    "print_every_n = 50\n",
    "epochs = 5\n",
    "save_every_steps = 500\n",
    "\n",
    "model = SherlockAI(len(create_set), lstmsize = lstmsize, learning_rate = learning_rate, \n",
    "                 batch_size = batch_size, num_steps = num_steps, num_layers = num_layers)\n",
    "\n",
    "counter= 0\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 100)\n",
    "initialized = tf.global_variables_initializer()\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    \n",
    "    sess.run(initialized)\n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            #print(shape)\n",
    "            #print(len(shape))\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "            #print(dim)\n",
    "                variable_parameters *= dim.value\n",
    "        \n",
    "        #print(variable_parameters)\n",
    "            total_parameters += variable_parameters\n",
    "        print(total_parameters)\n",
    "        \n",
    "        feed_state = sess.run(model.initial_state)\n",
    "    \n",
    "        \n",
    "        for x, y in get_batches(encoded_book, batch_size, num_steps):\n",
    "        \n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            \n",
    "            feed = {model.inputs: x, model.targets : y, model.keep_prob: keep_prob, model.initial_state: feed_state}\n",
    "            \n",
    "            batch_loss, feed_state, _ = sess.run([model.loss, model.final_state, model.optimizer], feed_dict = feed)\n",
    "            \n",
    "            total_parameters = 0\n",
    "            \n",
    "            \n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                \n",
    "                print('epochs: {}/{}..... '.format(i + 1, epochs),\n",
    "                     'training_step: {}.... '.format(counter),\n",
    "                      'training_loss: {:3f}....... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch '.format(end-start))\n",
    "                      \n",
    "               \n",
    "            if (counter % save_every_steps == 0):\n",
    "                \n",
    "                #saver.save(sess, 'checkpoints /{}___{}____{}.ckpt'.format(i +1 , counter, lstmsize))\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstmsize))\n",
    "                      \n",
    "                \n",
    "                      \n",
    "    #saver.save(sess, 'checkpoints /{}___{}____{}//final.ckpt'.format(i + 1, counter, lstmsize))\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstmsize))\n",
    "                      \n",
    "    samp = sample(checkpoint, 1000, lstmsize, len(create_set), prime=\"The\")\n",
    "    print(samp)        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=2):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = dict(enumerate(create_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstmsize, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = SherlockAI(len(create_set), lstmsize=lstmsize, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        feed_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = dict_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(create_set))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(create_set))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 500, lstmsize, len(create_set), prime=\"Sherlock\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
