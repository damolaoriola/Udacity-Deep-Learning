{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3864202"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting features from text\n",
    "\n",
    "with open('cano.txt', 'r') as f:\n",
    "    book = f.read()\n",
    "\n",
    "create_set = sorted(set(book))\n",
    "\n",
    "dict_int = {word: inte for inte, word in enumerate(create_set)}\n",
    "\n",
    "\n",
    "#create array of entire book\n",
    "encoded_book = np.array([dict_int[word] for word in book], dtype = np.int32)\n",
    "\n",
    "len(encoded_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          CHAPTER I\n",
      "          Mr. Sherlock Holmes\n",
      "\n",
      "\n",
      "     In the year 1878 I took my degree of Docto\n",
      "[ 0  1  1  1  1  1  1  1  1  1  1 28 33 26 41 45 30 43  1 34  0  1  1  1\n",
      "  1  1  1  1  1  1  1 38 72 11  1 44 62 59 72 66 69 57 65  1 33 69 66 67\n",
      " 59 73  0  0  0  1  1  1  1  1 34 68  1 74 62 59  1 79 59 55 72  1 14 21\n",
      " 20 21  1 34  1 74 69 69 65  1 67 79  1 58 59 61 72 59 59  1 69 60  1 29\n",
      " 69 57 74 69]\n"
     ]
    }
   ],
   "source": [
    "len(create_set)\n",
    "\n",
    "print(book[:100])\n",
    "\n",
    "'\\n' '\\n'\n",
    "\n",
    "print(encoded_book[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for gettin batches\n",
    "\n",
    "def get_batches(arr, batch_size, num_steps):\n",
    "\n",
    "    character_per_batch = batch_size * num_steps\n",
    "\n",
    "    num_batches = len(arr)//character_per_batch\n",
    "\n",
    "\n",
    "    #keep only enough to make full batches\n",
    "\n",
    "    arr = arr[: (character_per_batch * num_batches)]\n",
    "\n",
    "    #reshape into batch_size\n",
    "\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "\n",
    "\n",
    "    #split into x & y\n",
    "\n",
    "    for step in range(0, arr.shape[1], num_steps):\n",
    "\n",
    "        x = arr[:, step : step + num_steps]\n",
    "\n",
    "        y_temp = arr[:, (step + 1): (step+1) + num_steps]\n",
    "    \n",
    "        y = np.zeros(x.shape, dtype = x.dtype)\n",
    "    \n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "    \n",
    "        yield x, y\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  1  1  1  1  1  1  1  1  1 28]\n",
      " [74 73 69 68 11  1 48 59  1 73 62 55]\n",
      " [69 73 74  1 57 69 67 59  1 74 69  1]\n",
      " [ 9  1 55 68 58  1 62 69 77  1 62 59]\n",
      " [63 68  1 74 69 10 67 69 72 72 69 77]\n",
      " [72 63 68 61  1 63 68  1 74 62 59  1]\n",
      " [58  1 69 60  1 55  1 67 55 72 72 63]\n",
      " [60 63 57 63 55 66  1 59 78 70 69 68]\n",
      " [59 55 72 73 11  0  0  1  1  1  1  1]\n",
      " [ 1 56 59  1 74 72 75 73 74 59 58 11]]\n",
      "\n",
      " [[ 1  1  1  1  1  1  1  1  1  1 28 33]\n",
      " [73 69 68 11  1 48 59  1 73 62 55 66]\n",
      " [73 74  1 57 69 67 59  1 74 69  1 56]\n",
      " [ 1 55 68 58  1 62 69 77  1 62 59  1]\n",
      " [68  1 74 69 10 67 69 72 72 69 77  9]\n",
      " [63 68 61  1 63 68  1 74 62 59  1 61]\n",
      " [ 1 69 60  1 55  1 67 55 72 72 63 59]\n",
      " [63 57 63 55 66  1 59 78 70 69 68 59]\n",
      " [55 72 73 11  0  0  1  1  1  1  1  3]\n",
      " [56 59  1 74 72 75 73 74 59 58 11  1]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded_book, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print(x[:12, :12])\n",
    "print('\\n',y[:12, :12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input, output & keep_prob\n",
    "\n",
    "def tensor_variables(batch_size, num_steps):\n",
    "\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LSTM Cell\n",
    "\n",
    "def LSTM(lstm_size, batch_size, keep_prob, num_layers):\n",
    "\n",
    "    #Build lstm cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "    \n",
    "        Lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "        drop = tf.contrib.rnn.DropoutWrapper(Lstm, output_keep_prob = keep_prob)\n",
    "        \n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    multi_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    initial_state = multi_lstm.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return multi_lstm, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating predictions from our network\n",
    "\n",
    "def logits(lstm_output, lstm_size, outclasses_size ):\n",
    "\n",
    "\n",
    "    lstm_batch_list = tf.concat(lstm_output, axis =1)\n",
    "    \n",
    "    lstm_output = tf.reshape(lstm_batch_list, [-1, lstm_size])\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope ('softmax'):\n",
    "        \n",
    "        softmax_w = tf.get_variable('softmax_w', [lstm_size, outclasses_size], initializer = tf.contrib.layers.xavier_initializer(seed =1))\n",
    "        \n",
    "        softmax_b = tf.get_variable('softmax_b', [outclasses_size], initializer = tf.zeros_initializer())\n",
    "        \n",
    "    \n",
    "    logits = tf.add(tf.matmul(lstm_output, softmax_w), softmax_b)\n",
    "    \n",
    "    \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss\n",
    "\n",
    "def loss_hot(logits, targets, num_classes):\n",
    "    \n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    \n",
    "    y = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(learning_rate, grad_clip, loss):\n",
    "\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PROCESS\n",
    "\n",
    "class SherlockAI:\n",
    "    \n",
    "    def __init__(self, num_classes, lstmsize = 128, learning_rate = 0.01, \n",
    "                 batch_size = 64, num_steps = 50, num_layers = 2, \n",
    "                  sampling = False, grad_clip=5):\n",
    "    \n",
    "    \n",
    "        if sampling == True:\n",
    "        \n",
    "            batch_size, num_steps = 1, 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #Build the input placeholders\n",
    "        self.inputs, self.targets, self.keep_prob = tensor_variables(batch_size, num_steps)\n",
    "        \n",
    "        #Build LSTM Cell architecture                                                             \n",
    "        lstm, self.initial_state =  LSTM(lstmsize, batch_size, keep_prob, num_layers)\n",
    "        \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        #Run inputs through LSTM Cell\n",
    "        \n",
    "        outputs, final_state = tf.nn.dynamic_rnn(lstm, x_one_hot, initial_state = self.initial_state)\n",
    "        \n",
    "        self.final_state = final_state\n",
    "        \n",
    "        #Predictions using lstm run\n",
    "        \n",
    "        self.logits, self.predictions =  logits(outputs, lstmsize, num_classes)\n",
    "        \n",
    "        #loss function & optimizer\n",
    "        \n",
    "        self.loss = loss_hot(self.logits, self.targets, num_classes)\n",
    "        self.optimizer = build_optimizer(learning_rate, grad_clip, self.loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstmsize = 540       # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.4        # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-049f5830fe9f>:9: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-049f5830fe9f>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-c20c7a4c10bf>:30: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Damola\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Damola\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-9-3ab671c62952>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training phase\n",
    "\n",
    "print_every_n = 50\n",
    "epochs = 20\n",
    "save_every_steps = 500\n",
    "\n",
    "model = SherlockAI(len(create_set), lstmsize = lstmsize, learning_rate = learning_rate, \n",
    "                 batch_size = batch_size, num_steps = num_steps, num_layers = num_layers)\n",
    "\n",
    "counter= 0\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 100)\n",
    "initialized = tf.global_variables_initializer()\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    \n",
    "    sess.run(initialized)\n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            #print(shape)\n",
    "            #print(len(shape))\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "            #print(dim)\n",
    "                variable_parameters *= dim.value\n",
    "        \n",
    "        #print(variable_parameters)\n",
    "            total_parameters += variable_parameters\n",
    "        print(total_parameters)\n",
    "        \n",
    "        feed_state = sess.run(model.initial_state)\n",
    "    \n",
    "        \n",
    "        for x, y in get_batches(encoded_book, batch_size, num_steps):\n",
    "        \n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            \n",
    "            feed = {model.inputs: x, model.targets : y, model.keep_prob: keep_prob, model.initial_state: feed_state}\n",
    "            \n",
    "            batch_loss, feed_state, _ = sess.run([model.loss, model.final_state, model.optimizer], feed_dict = feed)\n",
    "            \n",
    "            total_parameters = 0\n",
    "            \n",
    "            \n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                \n",
    "                print('epochs: {}/{}..... '.format(i + 1, epochs),\n",
    "                     'training_step: {}.... '.format(counter),\n",
    "                      'training_loss: {:3f}....... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch '.format(end-start))\n",
    "                      \n",
    "               \n",
    "            if (counter % save_every_steps == 0):\n",
    "                \n",
    "                #saver.save(sess, 'checkpoints /{}___{}____{}.ckpt'.format(i +1 , counter, lstmsize))\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_k{}.ckpt\".format(counter, lstmsize, 0.4))\n",
    "                      \n",
    "                \n",
    "                      \n",
    "    #saver.save(sess, 'checkpoints /{}___{}____{}//final.ckpt'.format(i + 1, counter, lstmsize))\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}_k{}.ckpt\".format(counter, lstmsize, 0.4))\n",
    "                      \n",
    "    #samp = sample(checkpoint, 1000, lstmsize, len(create_set), prime=\"The\")\n",
    "    #print(samp)        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=2):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = dict(enumerate(create_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstmsize, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = SherlockAI(len(create_set), lstmsize=lstmsize, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        feed_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = dict_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(create_set))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(create_set))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = 'checkpoints\\\\i3000_l540.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstmsize, len(create_set), prime=\"the\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
