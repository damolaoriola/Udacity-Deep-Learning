{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3864202"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting features from text\n",
    "\n",
    "with open('cano.txt', 'r') as f:\n",
    "    book = f.read()\n",
    "\n",
    "create_set = sorted(set(book))\n",
    "\n",
    "dict_int = {word: inte for inte, word in enumerate(create_set)}\n",
    "\n",
    "\n",
    "#create array of entire book\n",
    "encoded_book = np.array([dict_int[word] for word in book], dtype = np.int32)\n",
    "\n",
    "len(encoded_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          CHAPTER I\n",
      "          Mr. Sherlock Holmes\n",
      "\n",
      "\n",
      "     In the year 1878 I took my degree of Docto\n",
      "[ 0  1  1  1  1  1  1  1  1  1  1 28 33 26 41 45 30 43  1 34  0  1  1  1\n",
      "  1  1  1  1  1  1  1 38 72 11  1 44 62 59 72 66 69 57 65  1 33 69 66 67\n",
      " 59 73  0  0  0  1  1  1  1  1 34 68  1 74 62 59  1 79 59 55 72  1 14 21\n",
      " 20 21  1 34  1 74 69 69 65  1 67 79  1 58 59 61 72 59 59  1 69 60  1 29\n",
      " 69 57 74 69]\n"
     ]
    }
   ],
   "source": [
    "len(create_set)\n",
    "\n",
    "print(book[:100])\n",
    "\n",
    "'\\n' '\\n'\n",
    "\n",
    "print(encoded_book[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for gettin batches\n",
    "\n",
    "def get_batches(arr, batch_size, num_steps):\n",
    "\n",
    "    character_per_batch = batch_size * num_steps\n",
    "\n",
    "    num_batches = len(arr)//character_per_batch\n",
    "\n",
    "\n",
    "    #keep only enough to make full batches\n",
    "\n",
    "    arr = arr[: (character_per_batch * num_batches)]\n",
    "\n",
    "    #reshape into batch_size\n",
    "\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "\n",
    "\n",
    "    #split into x & y\n",
    "\n",
    "    for step in range(0, arr.shape[1], num_steps):\n",
    "\n",
    "        x = arr[:, step : step + num_steps]\n",
    "\n",
    "        y_temp = arr[:, (step + 1): (step+1) + num_steps]\n",
    "    \n",
    "        y = np.zeros(x.shape, dtype = x.dtype)\n",
    "    \n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "    \n",
    "        yield x, y\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  1  1  1  1  1  1  1  1  1 28]\n",
      " [74 73 69 68 11  1 48 59  1 73 62 55]\n",
      " [69 73 74  1 57 69 67 59  1 74 69  1]\n",
      " [ 9  1 55 68 58  1 62 69 77  1 62 59]\n",
      " [63 68  1 74 69 10 67 69 72 72 69 77]\n",
      " [72 63 68 61  1 63 68  1 74 62 59  1]\n",
      " [58  1 69 60  1 55  1 67 55 72 72 63]\n",
      " [60 63 57 63 55 66  1 59 78 70 69 68]\n",
      " [59 55 72 73 11  0  0  1  1  1  1  1]\n",
      " [ 1 56 59  1 74 72 75 73 74 59 58 11]]\n",
      "\n",
      " [[ 1  1  1  1  1  1  1  1  1  1 28 33]\n",
      " [73 69 68 11  1 48 59  1 73 62 55 66]\n",
      " [73 74  1 57 69 67 59  1 74 69  1 56]\n",
      " [ 1 55 68 58  1 62 69 77  1 62 59  1]\n",
      " [68  1 74 69 10 67 69 72 72 69 77  9]\n",
      " [63 68 61  1 63 68  1 74 62 59  1 61]\n",
      " [ 1 69 60  1 55  1 67 55 72 72 63 59]\n",
      " [63 57 63 55 66  1 59 78 70 69 68 59]\n",
      " [55 72 73 11  0  0  1  1  1  1  1  3]\n",
      " [56 59  1 74 72 75 73 74 59 58 11  1]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded_book, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print(x[:12, :12])\n",
    "print('\\n',y[:12, :12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input, output & keep_prob\n",
    "\n",
    "def tensor_variables(batch_size, num_steps):\n",
    "\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LSTM Cell\n",
    "\n",
    "def LSTM(lstm_size, batch_size, keep_prob, num_layers):\n",
    "\n",
    "    #Build lstm cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "    \n",
    "        Lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "        drop = tf.contrib.rnn.DropoutWrapper(Lstm, output_keep_prob = keep_prob)\n",
    "        \n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    multi_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    initial_state = multi_lstm.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return multi_lstm, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating predictions from our network\n",
    "\n",
    "def logits(lstm_output, lstm_size, outclasses_size ):\n",
    "\n",
    "\n",
    "    lstm_batch_list = tf.concat(lstm_output, axis =1)\n",
    "    \n",
    "    lstm_output = tf.reshape(lstm_batch_list, [-1, lstm_size])\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope ('softmax'):\n",
    "        \n",
    "        softmax_w = tf.get_variable('softmax_w', [lstm_size, outclasses_size], initializer = tf.contrib.layers.xavier_initializer(seed =1))\n",
    "        \n",
    "        softmax_b = tf.get_variable('softmax_b', [outclasses_size], initializer = tf.zeros_initializer())\n",
    "        \n",
    "    \n",
    "    logits = tf.add(tf.matmul(lstm_output, softmax_w), softmax_b)\n",
    "    \n",
    "    \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss\n",
    "\n",
    "def loss_hot(logits, targets, num_classes):\n",
    "    \n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    \n",
    "    y = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(learning_rate, grad_clip, loss):\n",
    "\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PROCESS\n",
    "\n",
    "class SherlockAI:\n",
    "    \n",
    "    def __init__(self, num_classes, lstmsize = 128, learning_rate = 0.01, \n",
    "                 batch_size = 64, num_steps = 50, num_layers = 2, \n",
    "                  sampling = False, grad_clip=5):\n",
    "    \n",
    "    \n",
    "        if sampling == True:\n",
    "        \n",
    "            batch_size, num_steps = 1, 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #Build the input placeholders\n",
    "        self.inputs, self.targets, self.keep_prob = tensor_variables(batch_size, num_steps)\n",
    "        \n",
    "        #Build LSTM Cell architecture                                                             \n",
    "        lstm, self.initial_state =  LSTM(lstmsize, batch_size, keep_prob, num_layers)\n",
    "        \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        #Run inputs through LSTM Cell\n",
    "        \n",
    "        outputs, final_state = tf.nn.dynamic_rnn(lstm, x_one_hot, initial_state = self.initial_state)\n",
    "        \n",
    "        self.final_state = final_state\n",
    "        \n",
    "        #Predictions using lstm run\n",
    "        \n",
    "        self.logits, self.predictions =  logits(outputs, lstmsize, num_classes)\n",
    "        \n",
    "        #loss function & optimizer\n",
    "        \n",
    "        self.loss = loss_hot(self.logits, self.targets, num_classes)\n",
    "        self.optimizer = build_optimizer(learning_rate, grad_clip, self.loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstmsize = 540       # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.2        # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3776321\n",
      "epochs: 1/5.....  training_step: 50....  training_loss: 3.076138.......  2.2098 sec/batch \n",
      "epochs: 1/5.....  training_step: 100....  training_loss: 2.943126.......  2.0150 sec/batch \n",
      "epochs: 1/5.....  training_step: 150....  training_loss: 2.878319.......  2.0984 sec/batch \n",
      "epochs: 1/5.....  training_step: 200....  training_loss: 2.801064.......  2.0795 sec/batch \n",
      "epochs: 1/5.....  training_step: 250....  training_loss: 2.607001.......  2.1277 sec/batch \n",
      "epochs: 1/5.....  training_step: 300....  training_loss: 2.523292.......  2.1364 sec/batch \n",
      "epochs: 1/5.....  training_step: 350....  training_loss: 2.369559.......  2.2309 sec/batch \n",
      "epochs: 1/5.....  training_step: 400....  training_loss: 2.327029.......  2.0571 sec/batch \n",
      "epochs: 1/5.....  training_step: 450....  training_loss: 2.265376.......  2.0410 sec/batch \n",
      "epochs: 1/5.....  training_step: 500....  training_loss: 2.272591.......  2.0481 sec/batch \n",
      "epochs: 1/5.....  training_step: 550....  training_loss: 2.259239.......  2.0524 sec/batch \n",
      "epochs: 1/5.....  training_step: 600....  training_loss: 2.206642.......  2.0594 sec/batch \n",
      "epochs: 1/5.....  training_step: 650....  training_loss: 2.137990.......  2.4130 sec/batch \n",
      "epochs: 1/5.....  training_step: 700....  training_loss: 2.072309.......  2.1447 sec/batch \n",
      "epochs: 1/5.....  training_step: 750....  training_loss: 2.047165.......  2.0721 sec/batch \n",
      "epochs: 1/5.....  training_step: 800....  training_loss: 2.128676.......  2.0413 sec/batch \n",
      "epochs: 1/5.....  training_step: 850....  training_loss: 2.027342.......  1.9727 sec/batch \n",
      "epochs: 1/5.....  training_step: 900....  training_loss: 1.951156.......  2.0049 sec/batch \n",
      "epochs: 1/5.....  training_step: 950....  training_loss: 2.014303.......  1.9929 sec/batch \n",
      "epochs: 1/5.....  training_step: 1000....  training_loss: 2.025394.......  1.9266 sec/batch \n",
      "epochs: 1/5.....  training_step: 1050....  training_loss: 1.979658.......  1.9107 sec/batch \n",
      "epochs: 1/5.....  training_step: 1100....  training_loss: 1.914089.......  3.3636 sec/batch \n",
      "epochs: 1/5.....  training_step: 1150....  training_loss: 2.037611.......  5.5188 sec/batch \n",
      "epochs: 1/5.....  training_step: 1200....  training_loss: 1.950831.......  4.0422 sec/batch \n",
      "epochs: 1/5.....  training_step: 1250....  training_loss: 1.954586.......  2.1115 sec/batch \n",
      "epochs: 1/5.....  training_step: 1300....  training_loss: 1.905141.......  2.2202 sec/batch \n",
      "epochs: 1/5.....  training_step: 1350....  training_loss: 1.869865.......  4.5349 sec/batch \n",
      "epochs: 1/5.....  training_step: 1400....  training_loss: 1.918283.......  5.3323 sec/batch \n",
      "epochs: 1/5.....  training_step: 1450....  training_loss: 1.842809.......  1.9290 sec/batch \n",
      "epochs: 1/5.....  training_step: 1500....  training_loss: 1.820759.......  3.4237 sec/batch \n",
      "epochs: 1/5.....  training_step: 1550....  training_loss: 1.743131.......  2.8329 sec/batch \n",
      "epochs: 1/5.....  training_step: 1600....  training_loss: 1.855836.......  1.9368 sec/batch \n",
      "epochs: 1/5.....  training_step: 1650....  training_loss: 1.772192.......  1.9964 sec/batch \n",
      "epochs: 1/5.....  training_step: 1700....  training_loss: 1.939029.......  1.9963 sec/batch \n",
      "epochs: 1/5.....  training_step: 1750....  training_loss: 1.836010.......  2.3614 sec/batch \n",
      "epochs: 1/5.....  training_step: 1800....  training_loss: 1.760911.......  2.1192 sec/batch \n",
      "epochs: 1/5.....  training_step: 1850....  training_loss: 1.823324.......  2.0448 sec/batch \n",
      "epochs: 1/5.....  training_step: 1900....  training_loss: 1.795096.......  2.2629 sec/batch \n",
      "epochs: 1/5.....  training_step: 1950....  training_loss: 1.812729.......  2.0359 sec/batch \n",
      "epochs: 1/5.....  training_step: 2000....  training_loss: 1.870375.......  2.0327 sec/batch \n",
      "epochs: 1/5.....  training_step: 2050....  training_loss: 1.717350.......  2.0541 sec/batch \n",
      "epochs: 1/5.....  training_step: 2100....  training_loss: 1.803579.......  2.0365 sec/batch \n",
      "epochs: 1/5.....  training_step: 2150....  training_loss: 1.717235.......  2.0163 sec/batch \n",
      "epochs: 1/5.....  training_step: 2200....  training_loss: 1.795333.......  2.0201 sec/batch \n",
      "epochs: 1/5.....  training_step: 2250....  training_loss: 1.643651.......  2.1784 sec/batch \n",
      "epochs: 1/5.....  training_step: 2300....  training_loss: 1.629785.......  2.0422 sec/batch \n",
      "epochs: 1/5.....  training_step: 2350....  training_loss: 1.783183.......  2.0434 sec/batch \n",
      "epochs: 1/5.....  training_step: 2400....  training_loss: 1.705389.......  2.0176 sec/batch \n",
      "3776321\n",
      "epochs: 2/5.....  training_step: 2450....  training_loss: 1.672673.......  2.4410 sec/batch \n"
     ]
    }
   ],
   "source": [
    "#Training phase\n",
    "\n",
    "print_every_n = 50\n",
    "epochs = 5\n",
    "save_every_steps = 500\n",
    "\n",
    "model = SherlockAI(len(create_set), lstmsize = lstmsize, learning_rate = learning_rate, \n",
    "                 batch_size = batch_size, num_steps = num_steps, num_layers = num_layers)\n",
    "\n",
    "counter= 0\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 100)\n",
    "initialized = tf.global_variables_initializer()\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    \n",
    "    sess.run(initialized)\n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            #print(shape)\n",
    "            #print(len(shape))\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "            #print(dim)\n",
    "                variable_parameters *= dim.value\n",
    "        \n",
    "        #print(variable_parameters)\n",
    "            total_parameters += variable_parameters\n",
    "        print(total_parameters)\n",
    "        \n",
    "        feed_state = sess.run(model.initial_state)\n",
    "    \n",
    "        \n",
    "        for x, y in get_batches(encoded_book, batch_size, num_steps):\n",
    "        \n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            \n",
    "            feed = {model.inputs: x, model.targets : y, model.keep_prob: keep_prob, model.initial_state: feed_state}\n",
    "            \n",
    "            batch_loss, feed_state, _ = sess.run([model.loss, model.final_state, model.optimizer], feed_dict = feed)\n",
    "            \n",
    "            total_parameters = 0\n",
    "            \n",
    "            \n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                \n",
    "                print('epochs: {}/{}..... '.format(i + 1, epochs),\n",
    "                     'training_step: {}.... '.format(counter),\n",
    "                      'training_loss: {:3f}....... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch '.format(end-start))\n",
    "                      \n",
    "               \n",
    "            if (counter % save_every_steps == 0):\n",
    "                \n",
    "                #saver.save(sess, 'checkpoints /{}___{}____{}.ckpt'.format(i +1 , counter, lstmsize))\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstmsize))\n",
    "                      \n",
    "                \n",
    "                      \n",
    "    #saver.save(sess, 'checkpoints /{}___{}____{}//final.ckpt'.format(i + 1, counter, lstmsize))\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}_k{}.ckpt\".format(counter, lstmsize, 0.6))\n",
    "                      \n",
    "    #samp = sample(checkpoint, 1000, lstmsize, len(create_set), prime=\"The\")\n",
    "    #print(samp)        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=2):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = dict(enumerate(create_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstmsize, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = SherlockAI(len(create_set), lstmsize=lstmsize, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        feed_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = dict_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(create_set))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(create_set))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i7500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i4000_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i4500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i5000_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i5500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i6000_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i6500_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i7000_l540.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i7500_l540.ckpt\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.train' has no attribute 'checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d0ba907613fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'checkpoints\\\\i500_l540.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.train' has no attribute 'checkpoints'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i12075_l540.ckpt\n",
      "ther\n",
      "     light as he was than the thing and through the starter\n",
      "     of the streens and then he had sack to his than to the date\n",
      "     that the window had a stranke on himself to the trange of\n",
      "     the tome of the shall and starred to her the tame.\n",
      "\n",
      "     \"It was the thill of the tratk of him and see that the sighost was strong a\n",
      "     hordy that he was thow whan I am shere to my hands and to his\n",
      "     town and the halrs of the morning to the stails of the case of\n",
      "     thas man, to his hand of a singerous an amention. \"It was not in the\n",
      "     sawer in the matter of them to his fasher. I should not are\n",
      "     stainting. It was the passages, who was an and had to be to take\n",
      "     a string and sealling, but we the parsable one of the station, and the\n",
      "     latter would have to the poor than an already toow a strange and\n",
      "     case of thcuetly of the amence there in two\n",
      "     candour the plood. The seemed on to the stopy and stating a\n",
      "     streat of the station of any ownired the for that his face of hi\n"
     ]
    }
   ],
   "source": [
    "#checkpoint = 'checkpoints\\\\i3000_l540.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstmsize, len(create_set), prime=\"the\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
