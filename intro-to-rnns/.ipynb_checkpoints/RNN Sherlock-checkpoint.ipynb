{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3864202"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting features from text\n",
    "\n",
    "with open('cano.txt', 'r') as f:\n",
    "    book = f.read()\n",
    "\n",
    "create_set = sorted(set(book))\n",
    "\n",
    "dict_int = {word: inte for inte, word in enumerate(create_set)}\n",
    "\n",
    "\n",
    "#create array of entire book\n",
    "encoded_book = np.array([dict_int[word] for word in book], dtype = np.int32)\n",
    "\n",
    "len(encoded_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          CHAPTER I\n",
      "          Mr. Sherlock Holmes\n",
      "\n",
      "\n",
      "     In the year 1878 I took my degree of Docto\n",
      "[ 0  1  1  1  1  1  1  1  1  1  1 28 33 26 41 45 30 43  1 34  0  1  1  1\n",
      "  1  1  1  1  1  1  1 38 72 11  1 44 62 59 72 66 69 57 65  1 33 69 66 67\n",
      " 59 73  0  0  0  1  1  1  1  1 34 68  1 74 62 59  1 79 59 55 72  1 14 21\n",
      " 20 21  1 34  1 74 69 69 65  1 67 79  1 58 59 61 72 59 59  1 69 60  1 29\n",
      " 69 57 74 69]\n"
     ]
    }
   ],
   "source": [
    "len(create_set)\n",
    "\n",
    "print(book[:100])\n",
    "\n",
    "'\\n' '\\n'\n",
    "\n",
    "print(encoded_book[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for gettin batches\n",
    "\n",
    "def get_batches(arr, batch_size, num_steps):\n",
    "\n",
    "    character_per_batch = batch_size * num_steps\n",
    "\n",
    "    num_batches = len(arr)//character_per_batch\n",
    "\n",
    "\n",
    "    #keep only enough to make full batches\n",
    "\n",
    "    arr = arr[: (character_per_batch * num_batches)]\n",
    "\n",
    "    #reshape into batch_size\n",
    "\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "\n",
    "\n",
    "    #split into x & y\n",
    "\n",
    "    for step in range(0, arr.shape[1], num_steps):\n",
    "\n",
    "        x = arr[:, step : step + num_steps]\n",
    "\n",
    "        y_temp = arr[:, (step + 1): (step+1) + num_steps]\n",
    "    \n",
    "        y = np.zeros(x.shape, dtype = x.dtype)\n",
    "    \n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "    \n",
    "        yield x, y\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  1  1  1  1  1  1  1  1  1 28]\n",
      " [74 73 69 68 11  1 48 59  1 73 62 55]\n",
      " [69 73 74  1 57 69 67 59  1 74 69  1]\n",
      " [ 9  1 55 68 58  1 62 69 77  1 62 59]\n",
      " [63 68  1 74 69 10 67 69 72 72 69 77]\n",
      " [72 63 68 61  1 63 68  1 74 62 59  1]\n",
      " [58  1 69 60  1 55  1 67 55 72 72 63]\n",
      " [60 63 57 63 55 66  1 59 78 70 69 68]\n",
      " [59 55 72 73 11  0  0  1  1  1  1  1]\n",
      " [ 1 56 59  1 74 72 75 73 74 59 58 11]]\n",
      "\n",
      " [[ 1  1  1  1  1  1  1  1  1  1 28 33]\n",
      " [73 69 68 11  1 48 59  1 73 62 55 66]\n",
      " [73 74  1 57 69 67 59  1 74 69  1 56]\n",
      " [ 1 55 68 58  1 62 69 77  1 62 59  1]\n",
      " [68  1 74 69 10 67 69 72 72 69 77  9]\n",
      " [63 68 61  1 63 68  1 74 62 59  1 61]\n",
      " [ 1 69 60  1 55  1 67 55 72 72 63 59]\n",
      " [63 57 63 55 66  1 59 78 70 69 68 59]\n",
      " [55 72 73 11  0  0  1  1  1  1  1  3]\n",
      " [56 59  1 74 72 75 73 74 59 58 11  1]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded_book, 10, 50)\n",
    "x, y = next(batches)\n",
    "\n",
    "print(x[:12, :12])\n",
    "print('\\n',y[:12, :12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input, output & keep_prob\n",
    "\n",
    "def tensor_variables(batch_size, num_steps):\n",
    "\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    return inputs, targets, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LSTM Cell\n",
    "\n",
    "def LSTM(lstm_size, batch_size, keep_prob, num_layers):\n",
    "\n",
    "    #Build lstm cell\n",
    "    \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "    \n",
    "        Lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "        drop = tf.contrib.rnn.DropoutWrapper(Lstm, output_keep_prob = keep_prob)\n",
    "        \n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    multi_lstm = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    initial_state = multi_lstm.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return multi_lstm, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating predictions from our network\n",
    "\n",
    "def logits(lstm_output, lstm_size, outclasses_size ):\n",
    "\n",
    "\n",
    "    lstm_batch_list = tf.concat(lstm_output, axis =1)\n",
    "    \n",
    "    lstm_output = tf.reshape(lstm_batch_list, [-1, lstm_size])\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope ('softmax'):\n",
    "        \n",
    "        softmax_w = tf.get_variable('softmax_w', [lstm_size, outclasses_size], initializer = tf.contrib.layers.xavier_initializer(seed =1))\n",
    "        \n",
    "        softmax_b = tf.get_variable('softmax_b', [outclasses_size], initializer = tf.zeros_initializer())\n",
    "        \n",
    "    \n",
    "    logits = tf.add(tf.matmul(lstm_output, softmax_w), softmax_b)\n",
    "    \n",
    "    \n",
    "    predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    return logits, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss\n",
    "\n",
    "def loss_hot(logits, targets, num_classes):\n",
    "    \n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    \n",
    "    y = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(learning_rate, grad_clip, loss):\n",
    "\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    return optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PROCESS\n",
    "\n",
    "class SherlockAI:\n",
    "    \n",
    "    def __init__(self, num_classes, lstmsize = 128, learning_rate = 0.01, \n",
    "                 batch_size = 64, num_steps = 50, num_layers = 2, \n",
    "                  sampling = False, grad_clip=5):\n",
    "    \n",
    "    \n",
    "        if sampling == True:\n",
    "        \n",
    "            batch_size, num_steps = 1, 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #Build the input placeholders\n",
    "        self.inputs, self.targets, self.keep_prob = tensor_variables(batch_size, num_steps)\n",
    "        \n",
    "        #Build LSTM Cell architecture                                                             \n",
    "        lstm, self.initial_state =  LSTM(lstmsize, batch_size, keep_prob, num_layers)\n",
    "        \n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        #Run inputs through LSTM Cell\n",
    "        \n",
    "        outputs, final_state = tf.nn.dynamic_rnn(lstm, x_one_hot, initial_state = self.initial_state)\n",
    "        \n",
    "        self.final_state = final_state\n",
    "        \n",
    "        #Predictions using lstm run\n",
    "        \n",
    "        self.logits, self.predictions =  logits(outputs, lstmsize, num_classes)\n",
    "        \n",
    "        #loss function & optimizer\n",
    "        \n",
    "        self.loss = loss_hot(self.logits, self.targets, num_classes)\n",
    "        self.optimizer = build_optimizer(learning_rate, grad_clip, self.loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32         # Sequences per batch\n",
    "num_steps = 50          # Number of sequence steps per batch\n",
    "lstmsize = 540       # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5        # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3776321\n",
      "epochs: 1/30.....  training_step: 50....  training_loss: 3.042669.......  1.8792 sec/batch \n",
      "epochs: 1/30.....  training_step: 100....  training_loss: 2.985367.......  1.8928 sec/batch \n",
      "epochs: 1/30.....  training_step: 150....  training_loss: 2.893414.......  1.8912 sec/batch \n",
      "epochs: 1/30.....  training_step: 200....  training_loss: 2.767314.......  1.8800 sec/batch \n",
      "epochs: 1/30.....  training_step: 250....  training_loss: 2.488854.......  1.9064 sec/batch \n",
      "epochs: 1/30.....  training_step: 300....  training_loss: 2.392628.......  1.8823 sec/batch \n",
      "epochs: 1/30.....  training_step: 350....  training_loss: 2.288828.......  1.8679 sec/batch \n",
      "epochs: 1/30.....  training_step: 400....  training_loss: 2.225275.......  1.8643 sec/batch \n",
      "epochs: 1/30.....  training_step: 450....  training_loss: 2.172939.......  1.8790 sec/batch \n",
      "epochs: 1/30.....  training_step: 500....  training_loss: 2.185823.......  1.9343 sec/batch \n",
      "epochs: 1/30.....  training_step: 550....  training_loss: 2.125424.......  1.8596 sec/batch \n",
      "epochs: 1/30.....  training_step: 600....  training_loss: 2.112941.......  1.8410 sec/batch \n",
      "epochs: 1/30.....  training_step: 650....  training_loss: 2.026718.......  1.8994 sec/batch \n",
      "epochs: 1/30.....  training_step: 700....  training_loss: 1.978339.......  1.8891 sec/batch \n",
      "epochs: 1/30.....  training_step: 750....  training_loss: 1.952718.......  1.9119 sec/batch \n",
      "epochs: 1/30.....  training_step: 800....  training_loss: 2.031650.......  1.8784 sec/batch \n",
      "epochs: 1/30.....  training_step: 850....  training_loss: 1.885193.......  1.8919 sec/batch \n",
      "epochs: 1/30.....  training_step: 900....  training_loss: 1.845324.......  1.8583 sec/batch \n",
      "epochs: 1/30.....  training_step: 950....  training_loss: 1.880242.......  1.8555 sec/batch \n",
      "epochs: 1/30.....  training_step: 1000....  training_loss: 1.864383.......  1.9132 sec/batch \n",
      "epochs: 1/30.....  training_step: 1050....  training_loss: 1.821255.......  1.8549 sec/batch \n",
      "epochs: 1/30.....  training_step: 1100....  training_loss: 1.740179.......  1.8593 sec/batch \n",
      "epochs: 1/30.....  training_step: 1150....  training_loss: 1.914961.......  1.8886 sec/batch \n",
      "epochs: 1/30.....  training_step: 1200....  training_loss: 1.796728.......  1.8787 sec/batch \n",
      "epochs: 1/30.....  training_step: 1250....  training_loss: 1.807208.......  1.9259 sec/batch \n",
      "epochs: 1/30.....  training_step: 1300....  training_loss: 1.753806.......  4.1886 sec/batch \n",
      "epochs: 1/30.....  training_step: 1350....  training_loss: 1.682890.......  1.9073 sec/batch \n",
      "epochs: 1/30.....  training_step: 1400....  training_loss: 1.813662.......  1.8759 sec/batch \n",
      "epochs: 1/30.....  training_step: 1450....  training_loss: 1.683776.......  1.8754 sec/batch \n",
      "epochs: 1/30.....  training_step: 1500....  training_loss: 1.679562.......  1.8814 sec/batch \n",
      "epochs: 1/30.....  training_step: 1550....  training_loss: 1.612581.......  3.6151 sec/batch \n",
      "epochs: 1/30.....  training_step: 1600....  training_loss: 1.689899.......  1.9379 sec/batch \n",
      "epochs: 1/30.....  training_step: 1650....  training_loss: 1.613935.......  2.0302 sec/batch \n",
      "epochs: 1/30.....  training_step: 1700....  training_loss: 1.786869.......  4.6733 sec/batch \n",
      "epochs: 1/30.....  training_step: 1750....  training_loss: 1.651256.......  1.9019 sec/batch \n",
      "epochs: 1/30.....  training_step: 1800....  training_loss: 1.580311.......  1.8668 sec/batch \n",
      "epochs: 1/30.....  training_step: 1850....  training_loss: 1.646350.......  2.5856 sec/batch \n",
      "epochs: 1/30.....  training_step: 1900....  training_loss: 1.625575.......  3.8471 sec/batch \n",
      "epochs: 1/30.....  training_step: 1950....  training_loss: 1.669063.......  1.8866 sec/batch \n",
      "epochs: 1/30.....  training_step: 2000....  training_loss: 1.682692.......  2.1416 sec/batch \n",
      "epochs: 1/30.....  training_step: 2050....  training_loss: 1.564293.......  1.8748 sec/batch \n",
      "epochs: 1/30.....  training_step: 2100....  training_loss: 1.619572.......  1.8619 sec/batch \n",
      "epochs: 1/30.....  training_step: 2150....  training_loss: 1.579940.......  1.8974 sec/batch \n",
      "epochs: 1/30.....  training_step: 2200....  training_loss: 1.626795.......  3.4994 sec/batch \n",
      "epochs: 1/30.....  training_step: 2250....  training_loss: 1.512521.......  3.7842 sec/batch \n",
      "epochs: 1/30.....  training_step: 2300....  training_loss: 1.480767.......  1.9010 sec/batch \n",
      "epochs: 1/30.....  training_step: 2350....  training_loss: 1.587491.......  1.8760 sec/batch \n",
      "epochs: 1/30.....  training_step: 2400....  training_loss: 1.565941.......  3.6010 sec/batch \n",
      "3776321\n",
      "epochs: 2/30.....  training_step: 2450....  training_loss: 1.511492.......  1.8747 sec/batch \n",
      "epochs: 2/30.....  training_step: 2500....  training_loss: 1.607838.......  1.8957 sec/batch \n",
      "epochs: 2/30.....  training_step: 2550....  training_loss: 1.613631.......  1.8884 sec/batch \n",
      "epochs: 2/30.....  training_step: 2600....  training_loss: 1.518676.......  1.8949 sec/batch \n",
      "epochs: 2/30.....  training_step: 2650....  training_loss: 1.582590.......  1.8891 sec/batch \n",
      "epochs: 2/30.....  training_step: 2700....  training_loss: 1.628109.......  4.7399 sec/batch \n",
      "epochs: 2/30.....  training_step: 2750....  training_loss: 1.583480.......  1.8806 sec/batch \n",
      "epochs: 2/30.....  training_step: 2800....  training_loss: 1.447204.......  1.9303 sec/batch \n",
      "epochs: 2/30.....  training_step: 2850....  training_loss: 1.496722.......  4.4147 sec/batch \n",
      "epochs: 2/30.....  training_step: 2900....  training_loss: 1.459932.......  1.8875 sec/batch \n",
      "epochs: 2/30.....  training_step: 2950....  training_loss: 1.462725.......  1.8905 sec/batch \n",
      "epochs: 2/30.....  training_step: 3000....  training_loss: 1.388079.......  1.8711 sec/batch \n",
      "epochs: 2/30.....  training_step: 3050....  training_loss: 1.516463.......  3.3206 sec/batch \n",
      "epochs: 2/30.....  training_step: 3100....  training_loss: 1.449785.......  1.8905 sec/batch \n",
      "epochs: 2/30.....  training_step: 3150....  training_loss: 1.403751.......  1.8788 sec/batch \n",
      "epochs: 2/30.....  training_step: 3200....  training_loss: 1.457013.......  4.3250 sec/batch \n",
      "epochs: 2/30.....  training_step: 3250....  training_loss: 1.438301.......  1.8751 sec/batch \n",
      "epochs: 2/30.....  training_step: 3300....  training_loss: 1.454840.......  1.8810 sec/batch \n",
      "epochs: 2/30.....  training_step: 3350....  training_loss: 1.396895.......  3.9549 sec/batch \n",
      "epochs: 2/30.....  training_step: 3400....  training_loss: 1.352041.......  3.9225 sec/batch \n",
      "epochs: 2/30.....  training_step: 3450....  training_loss: 1.430522.......  4.1219 sec/batch \n",
      "epochs: 2/30.....  training_step: 3500....  training_loss: 1.461396.......  1.8981 sec/batch \n",
      "epochs: 2/30.....  training_step: 3550....  training_loss: 1.426845.......  4.0358 sec/batch \n",
      "epochs: 2/30.....  training_step: 3600....  training_loss: 1.487460.......  1.8754 sec/batch \n",
      "epochs: 2/30.....  training_step: 3650....  training_loss: 1.421161.......  2.3443 sec/batch \n",
      "epochs: 2/30.....  training_step: 3700....  training_loss: 1.480842.......  2.0495 sec/batch \n",
      "epochs: 2/30.....  training_step: 3750....  training_loss: 1.341132.......  1.9149 sec/batch \n",
      "epochs: 2/30.....  training_step: 3800....  training_loss: 1.454530.......  2.3243 sec/batch \n",
      "epochs: 2/30.....  training_step: 3850....  training_loss: 1.368967.......  3.7557 sec/batch \n",
      "epochs: 2/30.....  training_step: 3900....  training_loss: 1.396870.......  1.8875 sec/batch \n",
      "epochs: 2/30.....  training_step: 3950....  training_loss: 1.470408.......  3.0003 sec/batch \n",
      "epochs: 2/30.....  training_step: 4000....  training_loss: 1.451454.......  1.8780 sec/batch \n",
      "epochs: 2/30.....  training_step: 4050....  training_loss: 1.368964.......  1.8617 sec/batch \n",
      "epochs: 2/30.....  training_step: 4100....  training_loss: 1.364217.......  1.9329 sec/batch \n",
      "epochs: 2/30.....  training_step: 4150....  training_loss: 1.303207.......  1.8975 sec/batch \n",
      "epochs: 2/30.....  training_step: 4200....  training_loss: 1.397226.......  1.8896 sec/batch \n",
      "epochs: 2/30.....  training_step: 4250....  training_loss: 1.356967.......  1.9254 sec/batch \n",
      "epochs: 2/30.....  training_step: 4300....  training_loss: 1.429302.......  1.9001 sec/batch \n",
      "epochs: 2/30.....  training_step: 4350....  training_loss: 1.374573.......  1.8960 sec/batch \n",
      "epochs: 2/30.....  training_step: 4400....  training_loss: 1.437146.......  1.8855 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 2/30.....  training_step: 4450....  training_loss: 1.345247.......  1.8847 sec/batch \n",
      "epochs: 2/30.....  training_step: 4500....  training_loss: 1.447028.......  1.9120 sec/batch \n",
      "epochs: 2/30.....  training_step: 4550....  training_loss: 1.364891.......  1.8892 sec/batch \n",
      "epochs: 2/30.....  training_step: 4600....  training_loss: 1.372769.......  1.8914 sec/batch \n",
      "epochs: 2/30.....  training_step: 4650....  training_loss: 1.357194.......  1.8998 sec/batch \n",
      "epochs: 2/30.....  training_step: 4700....  training_loss: 1.338223.......  1.9403 sec/batch \n",
      "epochs: 2/30.....  training_step: 4750....  training_loss: 1.329996.......  1.9227 sec/batch \n",
      "epochs: 2/30.....  training_step: 4800....  training_loss: 1.422820.......  1.9088 sec/batch \n",
      "3776321\n",
      "epochs: 3/30.....  training_step: 4850....  training_loss: 1.388840.......  1.8934 sec/batch \n",
      "epochs: 3/30.....  training_step: 4900....  training_loss: 1.351478.......  1.8638 sec/batch \n",
      "epochs: 3/30.....  training_step: 4950....  training_loss: 1.342241.......  1.8812 sec/batch \n",
      "epochs: 3/30.....  training_step: 5000....  training_loss: 1.331036.......  2.0392 sec/batch \n",
      "epochs: 3/30.....  training_step: 5050....  training_loss: 1.398254.......  1.8976 sec/batch \n",
      "epochs: 3/30.....  training_step: 5100....  training_loss: 1.381405.......  1.8978 sec/batch \n",
      "epochs: 3/30.....  training_step: 5150....  training_loss: 1.329155.......  1.8853 sec/batch \n",
      "epochs: 3/30.....  training_step: 5200....  training_loss: 1.365461.......  1.8950 sec/batch \n",
      "epochs: 3/30.....  training_step: 5250....  training_loss: 1.421620.......  1.8950 sec/batch \n",
      "epochs: 3/30.....  training_step: 5300....  training_loss: 1.380945.......  1.9332 sec/batch \n",
      "epochs: 3/30.....  training_step: 5350....  training_loss: 1.374009.......  1.9091 sec/batch \n",
      "epochs: 3/30.....  training_step: 5400....  training_loss: 1.278722.......  1.9004 sec/batch \n",
      "epochs: 3/30.....  training_step: 5450....  training_loss: 1.313856.......  1.8732 sec/batch \n",
      "epochs: 3/30.....  training_step: 5500....  training_loss: 1.344972.......  1.9146 sec/batch \n",
      "epochs: 3/30.....  training_step: 5550....  training_loss: 1.408405.......  1.8565 sec/batch \n",
      "epochs: 3/30.....  training_step: 5600....  training_loss: 1.312690.......  1.9038 sec/batch \n",
      "epochs: 3/30.....  training_step: 5650....  training_loss: 1.321241.......  1.8913 sec/batch \n",
      "epochs: 3/30.....  training_step: 5700....  training_loss: 1.262340.......  1.8576 sec/batch \n",
      "epochs: 3/30.....  training_step: 5750....  training_loss: 1.364058.......  1.9325 sec/batch \n",
      "epochs: 3/30.....  training_step: 5800....  training_loss: 1.267002.......  1.8774 sec/batch \n",
      "epochs: 3/30.....  training_step: 5850....  training_loss: 1.262507.......  1.8710 sec/batch \n",
      "epochs: 3/30.....  training_step: 5900....  training_loss: 1.301578.......  1.8837 sec/batch \n",
      "epochs: 3/30.....  training_step: 5950....  training_loss: 1.434465.......  1.8963 sec/batch \n",
      "epochs: 3/30.....  training_step: 6000....  training_loss: 1.372578.......  1.8994 sec/batch \n",
      "epochs: 3/30.....  training_step: 6050....  training_loss: 1.333785.......  1.8835 sec/batch \n",
      "epochs: 3/30.....  training_step: 6100....  training_loss: 1.278760.......  1.8665 sec/batch \n",
      "epochs: 3/30.....  training_step: 6150....  training_loss: 1.304720.......  1.8562 sec/batch \n",
      "epochs: 3/30.....  training_step: 6200....  training_loss: 1.394197.......  1.8579 sec/batch \n",
      "epochs: 3/30.....  training_step: 6250....  training_loss: 1.288669.......  2.0378 sec/batch \n",
      "epochs: 3/30.....  training_step: 6300....  training_loss: 1.339163.......  1.8592 sec/batch \n",
      "epochs: 3/30.....  training_step: 6350....  training_loss: 1.247560.......  1.8571 sec/batch \n",
      "epochs: 3/30.....  training_step: 6400....  training_loss: 1.253290.......  1.8750 sec/batch \n",
      "epochs: 3/30.....  training_step: 6450....  training_loss: 1.225387.......  1.9031 sec/batch \n",
      "epochs: 3/30.....  training_step: 6500....  training_loss: 1.340122.......  1.9096 sec/batch \n",
      "epochs: 3/30.....  training_step: 6550....  training_loss: 1.301626.......  1.8629 sec/batch \n",
      "epochs: 3/30.....  training_step: 6600....  training_loss: 1.322425.......  1.8639 sec/batch \n",
      "epochs: 3/30.....  training_step: 6650....  training_loss: 1.320852.......  1.8741 sec/batch \n",
      "epochs: 3/30.....  training_step: 6700....  training_loss: 1.327585.......  1.8598 sec/batch \n",
      "epochs: 3/30.....  training_step: 6750....  training_loss: 1.218796.......  1.9123 sec/batch \n",
      "epochs: 3/30.....  training_step: 6800....  training_loss: 1.326511.......  1.8566 sec/batch \n",
      "epochs: 3/30.....  training_step: 6850....  training_loss: 1.297472.......  2.1081 sec/batch \n",
      "epochs: 3/30.....  training_step: 6900....  training_loss: 1.283560.......  1.8734 sec/batch \n",
      "epochs: 3/30.....  training_step: 6950....  training_loss: 1.287949.......  1.9298 sec/batch \n",
      "epochs: 3/30.....  training_step: 7000....  training_loss: 1.301763.......  1.9622 sec/batch \n",
      "epochs: 3/30.....  training_step: 7050....  training_loss: 1.275910.......  1.9209 sec/batch \n",
      "epochs: 3/30.....  training_step: 7100....  training_loss: 1.336385.......  1.9785 sec/batch \n",
      "epochs: 3/30.....  training_step: 7150....  training_loss: 1.343275.......  1.9600 sec/batch \n",
      "epochs: 3/30.....  training_step: 7200....  training_loss: 1.326708.......  1.9516 sec/batch \n",
      "3776321\n",
      "epochs: 4/30.....  training_step: 7250....  training_loss: 1.242873.......  1.9604 sec/batch \n",
      "epochs: 4/30.....  training_step: 7300....  training_loss: 1.250693.......  1.9845 sec/batch \n",
      "epochs: 4/30.....  training_step: 7350....  training_loss: 1.360578.......  1.9408 sec/batch \n",
      "epochs: 4/30.....  training_step: 7400....  training_loss: 1.248984.......  1.9434 sec/batch \n",
      "epochs: 4/30.....  training_step: 7450....  training_loss: 1.305222.......  1.9390 sec/batch \n",
      "epochs: 4/30.....  training_step: 7500....  training_loss: 1.324912.......  1.9740 sec/batch \n",
      "epochs: 4/30.....  training_step: 7550....  training_loss: 1.332103.......  1.9465 sec/batch \n",
      "epochs: 4/30.....  training_step: 7600....  training_loss: 1.276433.......  1.9631 sec/batch \n",
      "epochs: 4/30.....  training_step: 7650....  training_loss: 1.276593.......  1.9890 sec/batch \n",
      "epochs: 4/30.....  training_step: 7700....  training_loss: 1.255019.......  1.9368 sec/batch \n",
      "epochs: 4/30.....  training_step: 7750....  training_loss: 1.325989.......  1.9559 sec/batch \n",
      "epochs: 4/30.....  training_step: 7800....  training_loss: 1.304234.......  1.9522 sec/batch \n",
      "epochs: 4/30.....  training_step: 7850....  training_loss: 1.255493.......  1.9928 sec/batch \n",
      "epochs: 4/30.....  training_step: 7900....  training_loss: 1.310912.......  1.9554 sec/batch \n",
      "epochs: 4/30.....  training_step: 7950....  training_loss: 1.316862.......  1.9423 sec/batch \n",
      "epochs: 4/30.....  training_step: 8000....  training_loss: 1.259373.......  1.9418 sec/batch \n",
      "epochs: 4/30.....  training_step: 8050....  training_loss: 1.275398.......  1.9996 sec/batch \n",
      "epochs: 4/30.....  training_step: 8100....  training_loss: 1.200747.......  2.0037 sec/batch \n",
      "epochs: 4/30.....  training_step: 8150....  training_loss: 1.256124.......  1.9454 sec/batch \n",
      "epochs: 4/30.....  training_step: 8200....  training_loss: 1.283228.......  1.9433 sec/batch \n",
      "epochs: 4/30.....  training_step: 8250....  training_loss: 1.226259.......  1.9590 sec/batch \n",
      "epochs: 4/30.....  training_step: 8300....  training_loss: 1.322104.......  1.9235 sec/batch \n",
      "epochs: 4/30.....  training_step: 8350....  training_loss: 1.250615.......  1.9575 sec/batch \n",
      "epochs: 4/30.....  training_step: 8400....  training_loss: 1.269083.......  1.9505 sec/batch \n",
      "epochs: 4/30.....  training_step: 8450....  training_loss: 1.238837.......  1.9243 sec/batch \n",
      "epochs: 4/30.....  training_step: 8500....  training_loss: 1.206962.......  1.9482 sec/batch \n",
      "epochs: 4/30.....  training_step: 8550....  training_loss: 1.242698.......  2.2184 sec/batch \n",
      "epochs: 4/30.....  training_step: 8600....  training_loss: 1.226454.......  1.8987 sec/batch \n",
      "epochs: 4/30.....  training_step: 8650....  training_loss: 1.252312.......  2.2440 sec/batch \n",
      "epochs: 4/30.....  training_step: 8700....  training_loss: 1.226202.......  1.9412 sec/batch \n",
      "epochs: 4/30.....  training_step: 8750....  training_loss: 1.261766.......  1.9245 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 4/30.....  training_step: 8800....  training_loss: 1.199042.......  1.9501 sec/batch \n",
      "epochs: 4/30.....  training_step: 8850....  training_loss: 1.200441.......  1.9624 sec/batch \n",
      "epochs: 4/30.....  training_step: 8900....  training_loss: 1.258798.......  1.9619 sec/batch \n",
      "epochs: 4/30.....  training_step: 8950....  training_loss: 1.269466.......  1.9309 sec/batch \n",
      "epochs: 4/30.....  training_step: 9000....  training_loss: 1.250501.......  1.9276 sec/batch \n",
      "epochs: 4/30.....  training_step: 9050....  training_loss: 1.244651.......  1.9318 sec/batch \n",
      "epochs: 4/30.....  training_step: 9100....  training_loss: 1.276949.......  1.9416 sec/batch \n",
      "epochs: 4/30.....  training_step: 9150....  training_loss: 1.277496.......  1.9418 sec/batch \n",
      "epochs: 4/30.....  training_step: 9200....  training_loss: 1.212591.......  1.9610 sec/batch \n",
      "epochs: 4/30.....  training_step: 9250....  training_loss: 1.317528.......  1.9151 sec/batch \n",
      "epochs: 4/30.....  training_step: 9300....  training_loss: 1.280271.......  1.9603 sec/batch \n",
      "epochs: 4/30.....  training_step: 9350....  training_loss: 1.285671.......  2.0097 sec/batch \n",
      "epochs: 4/30.....  training_step: 9400....  training_loss: 1.222638.......  1.9481 sec/batch \n",
      "epochs: 4/30.....  training_step: 9450....  training_loss: 1.252180.......  1.9702 sec/batch \n",
      "epochs: 4/30.....  training_step: 9500....  training_loss: 1.319977.......  1.9150 sec/batch \n",
      "epochs: 4/30.....  training_step: 9550....  training_loss: 1.212482.......  1.9656 sec/batch \n",
      "epochs: 4/30.....  training_step: 9600....  training_loss: 1.251065.......  1.9452 sec/batch \n",
      "epochs: 4/30.....  training_step: 9650....  training_loss: 1.185694.......  1.9362 sec/batch \n",
      "3776321\n",
      "epochs: 5/30.....  training_step: 9700....  training_loss: 1.189201.......  1.9352 sec/batch \n",
      "epochs: 5/30.....  training_step: 9750....  training_loss: 1.241181.......  2.0156 sec/batch \n",
      "epochs: 5/30.....  training_step: 9800....  training_loss: 1.138629.......  1.9424 sec/batch \n",
      "epochs: 5/30.....  training_step: 9850....  training_loss: 1.318825.......  1.9386 sec/batch \n",
      "epochs: 5/30.....  training_step: 9900....  training_loss: 1.271108.......  1.9514 sec/batch \n",
      "epochs: 5/30.....  training_step: 9950....  training_loss: 1.307833.......  1.9540 sec/batch \n",
      "epochs: 5/30.....  training_step: 10000....  training_loss: 1.377530.......  1.9358 sec/batch \n",
      "epochs: 5/30.....  training_step: 10050....  training_loss: 1.238826.......  1.9166 sec/batch \n",
      "epochs: 5/30.....  training_step: 10100....  training_loss: 1.306172.......  1.9222 sec/batch \n",
      "epochs: 5/30.....  training_step: 10150....  training_loss: 1.307324.......  1.9391 sec/batch \n",
      "epochs: 5/30.....  training_step: 10200....  training_loss: 1.247609.......  1.9598 sec/batch \n",
      "epochs: 5/30.....  training_step: 10250....  training_loss: 1.160044.......  1.9262 sec/batch \n",
      "epochs: 5/30.....  training_step: 10300....  training_loss: 1.216638.......  1.9363 sec/batch \n",
      "epochs: 5/30.....  training_step: 10350....  training_loss: 1.233263.......  1.9700 sec/batch \n",
      "epochs: 5/30.....  training_step: 10400....  training_loss: 1.201216.......  1.9305 sec/batch \n",
      "epochs: 5/30.....  training_step: 10450....  training_loss: 1.211753.......  1.9659 sec/batch \n",
      "epochs: 5/30.....  training_step: 10500....  training_loss: 1.117169.......  1.9251 sec/batch \n",
      "epochs: 5/30.....  training_step: 10550....  training_loss: 1.220818.......  1.9498 sec/batch \n",
      "epochs: 5/30.....  training_step: 10600....  training_loss: 1.163557.......  1.9333 sec/batch \n",
      "epochs: 5/30.....  training_step: 10650....  training_loss: 1.203378.......  1.9329 sec/batch \n",
      "epochs: 5/30.....  training_step: 10700....  training_loss: 1.192746.......  1.9768 sec/batch \n",
      "epochs: 5/30.....  training_step: 10750....  training_loss: 1.219869.......  1.9787 sec/batch \n",
      "epochs: 5/30.....  training_step: 10800....  training_loss: 1.222736.......  2.0974 sec/batch \n",
      "epochs: 5/30.....  training_step: 10850....  training_loss: 1.196700.......  1.8709 sec/batch \n",
      "epochs: 5/30.....  training_step: 10900....  training_loss: 1.279176.......  1.8677 sec/batch \n",
      "epochs: 5/30.....  training_step: 10950....  training_loss: 1.150946.......  1.8906 sec/batch \n",
      "epochs: 5/30.....  training_step: 11000....  training_loss: 1.304584.......  1.8712 sec/batch \n",
      "epochs: 5/30.....  training_step: 11050....  training_loss: 1.191001.......  1.8962 sec/batch \n",
      "epochs: 5/30.....  training_step: 11100....  training_loss: 1.176284.......  2.6595 sec/batch \n",
      "epochs: 5/30.....  training_step: 11150....  training_loss: 1.183908.......  1.9489 sec/batch \n",
      "epochs: 5/30.....  training_step: 11200....  training_loss: 1.204850.......  1.9736 sec/batch \n",
      "epochs: 5/30.....  training_step: 11250....  training_loss: 1.246789.......  2.0620 sec/batch \n",
      "epochs: 5/30.....  training_step: 11300....  training_loss: 1.279955.......  2.4233 sec/batch \n",
      "epochs: 5/30.....  training_step: 11350....  training_loss: 1.214132.......  2.1011 sec/batch \n",
      "epochs: 5/30.....  training_step: 11400....  training_loss: 1.264069.......  2.6125 sec/batch \n",
      "epochs: 5/30.....  training_step: 11450....  training_loss: 1.221302.......  1.9599 sec/batch \n",
      "epochs: 5/30.....  training_step: 11500....  training_loss: 1.261561.......  1.9439 sec/batch \n",
      "epochs: 5/30.....  training_step: 11550....  training_loss: 1.214154.......  1.9519 sec/batch \n",
      "epochs: 5/30.....  training_step: 11600....  training_loss: 1.219345.......  1.9469 sec/batch \n",
      "epochs: 5/30.....  training_step: 11650....  training_loss: 1.200600.......  1.9519 sec/batch \n",
      "epochs: 5/30.....  training_step: 11700....  training_loss: 1.227661.......  2.0348 sec/batch \n",
      "epochs: 5/30.....  training_step: 11750....  training_loss: 1.299737.......  2.0708 sec/batch \n",
      "epochs: 5/30.....  training_step: 11800....  training_loss: 1.262773.......  2.3217 sec/batch \n",
      "epochs: 5/30.....  training_step: 11850....  training_loss: 1.226079.......  1.9889 sec/batch \n",
      "epochs: 5/30.....  training_step: 11900....  training_loss: 1.213186.......  2.1148 sec/batch \n",
      "epochs: 5/30.....  training_step: 11950....  training_loss: 1.321325.......  2.0398 sec/batch \n",
      "epochs: 5/30.....  training_step: 12000....  training_loss: 1.193516.......  2.0608 sec/batch \n",
      "epochs: 5/30.....  training_step: 12050....  training_loss: 1.263062.......  2.0158 sec/batch \n",
      "3776321\n",
      "epochs: 6/30.....  training_step: 12100....  training_loss: 1.230607.......  2.0198 sec/batch \n",
      "epochs: 6/30.....  training_step: 12150....  training_loss: 1.208281.......  2.2157 sec/batch \n",
      "epochs: 6/30.....  training_step: 12200....  training_loss: 1.137189.......  2.1877 sec/batch \n",
      "epochs: 6/30.....  training_step: 12250....  training_loss: 1.260935.......  2.0118 sec/batch \n",
      "epochs: 6/30.....  training_step: 12300....  training_loss: 1.278033.......  2.1578 sec/batch \n",
      "epochs: 6/30.....  training_step: 12350....  training_loss: 1.198448.......  1.9979 sec/batch \n",
      "epochs: 6/30.....  training_step: 12400....  training_loss: 1.184806.......  2.0688 sec/batch \n",
      "epochs: 6/30.....  training_step: 12450....  training_loss: 1.293011.......  2.1128 sec/batch \n",
      "epochs: 6/30.....  training_step: 12500....  training_loss: 1.215141.......  2.0009 sec/batch \n",
      "epochs: 6/30.....  training_step: 12550....  training_loss: 1.230634.......  2.1118 sec/batch \n",
      "epochs: 6/30.....  training_step: 12600....  training_loss: 1.332461.......  2.3427 sec/batch \n",
      "epochs: 6/30.....  training_step: 12650....  training_loss: 1.184587.......  2.2667 sec/batch \n",
      "epochs: 6/30.....  training_step: 12700....  training_loss: 1.233276.......  2.0878 sec/batch \n",
      "epochs: 6/30.....  training_step: 12750....  training_loss: 1.128231.......  1.9979 sec/batch \n",
      "epochs: 6/30.....  training_step: 12800....  training_loss: 1.274641.......  2.2957 sec/batch \n",
      "epochs: 6/30.....  training_step: 12850....  training_loss: 1.151048.......  1.9889 sec/batch \n",
      "epochs: 6/30.....  training_step: 12900....  training_loss: 1.181563.......  2.0178 sec/batch \n",
      "epochs: 6/30.....  training_step: 12950....  training_loss: 1.253438.......  2.0268 sec/batch \n",
      "epochs: 6/30.....  training_step: 13000....  training_loss: 1.108297.......  1.9899 sec/batch \n",
      "epochs: 6/30.....  training_step: 13050....  training_loss: 1.200583.......  2.0298 sec/batch \n",
      "epochs: 6/30.....  training_step: 13100....  training_loss: 1.224226.......  2.0108 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 6/30.....  training_step: 13150....  training_loss: 1.197675.......  1.9989 sec/batch \n",
      "epochs: 6/30.....  training_step: 13200....  training_loss: 1.196665.......  1.9979 sec/batch \n",
      "epochs: 6/30.....  training_step: 13250....  training_loss: 1.191077.......  4.2886 sec/batch \n",
      "epochs: 6/30.....  training_step: 13300....  training_loss: 1.207798.......  1.9027 sec/batch \n",
      "epochs: 6/30.....  training_step: 13350....  training_loss: 1.201034.......  3.6785 sec/batch \n",
      "epochs: 6/30.....  training_step: 13400....  training_loss: 1.245377.......  1.9009 sec/batch \n",
      "epochs: 6/30.....  training_step: 13450....  training_loss: 1.179927.......  1.9068 sec/batch \n",
      "epochs: 6/30.....  training_step: 13500....  training_loss: 1.242732.......  3.7516 sec/batch \n",
      "epochs: 6/30.....  training_step: 13550....  training_loss: 1.206923.......  1.8749 sec/batch \n",
      "epochs: 6/30.....  training_step: 13600....  training_loss: 1.265343.......  1.8953 sec/batch \n",
      "epochs: 6/30.....  training_step: 13650....  training_loss: 1.271394.......  1.9865 sec/batch \n",
      "epochs: 6/30.....  training_step: 13700....  training_loss: 1.146158.......  1.8905 sec/batch \n",
      "epochs: 6/30.....  training_step: 13750....  training_loss: 1.146670.......  2.7870 sec/batch \n",
      "epochs: 6/30.....  training_step: 13800....  training_loss: 1.242463.......  3.9725 sec/batch \n",
      "epochs: 6/30.....  training_step: 13850....  training_loss: 1.249325.......  3.8129 sec/batch \n",
      "epochs: 6/30.....  training_step: 13900....  training_loss: 1.146600.......  1.8916 sec/batch \n",
      "epochs: 6/30.....  training_step: 13950....  training_loss: 1.197385.......  1.8784 sec/batch \n",
      "epochs: 6/30.....  training_step: 14000....  training_loss: 1.185870.......  4.5906 sec/batch \n",
      "epochs: 6/30.....  training_step: 14050....  training_loss: 1.229545.......  3.2943 sec/batch \n",
      "epochs: 6/30.....  training_step: 14100....  training_loss: 1.142786.......  1.9412 sec/batch \n",
      "epochs: 6/30.....  training_step: 14150....  training_loss: 1.229294.......  1.8795 sec/batch \n",
      "epochs: 6/30.....  training_step: 14200....  training_loss: 1.145361.......  1.9145 sec/batch \n",
      "epochs: 6/30.....  training_step: 14250....  training_loss: 1.197482.......  1.8797 sec/batch \n",
      "epochs: 6/30.....  training_step: 14300....  training_loss: 1.109650.......  1.8772 sec/batch \n",
      "epochs: 6/30.....  training_step: 14350....  training_loss: 1.191745.......  1.9199 sec/batch \n",
      "epochs: 6/30.....  training_step: 14400....  training_loss: 1.172602.......  1.8864 sec/batch \n",
      "epochs: 6/30.....  training_step: 14450....  training_loss: 1.215727.......  3.1198 sec/batch \n",
      "3776321\n",
      "epochs: 7/30.....  training_step: 14500....  training_loss: 1.262208.......  3.0235 sec/batch \n",
      "epochs: 7/30.....  training_step: 14550....  training_loss: 1.192145.......  1.9058 sec/batch \n",
      "epochs: 7/30.....  training_step: 14600....  training_loss: 1.234348.......  1.8847 sec/batch \n",
      "epochs: 7/30.....  training_step: 14650....  training_loss: 1.160460.......  1.8905 sec/batch \n",
      "epochs: 7/30.....  training_step: 14700....  training_loss: 1.265703.......  1.9009 sec/batch \n",
      "epochs: 7/30.....  training_step: 14750....  training_loss: 1.161866.......  4.7195 sec/batch \n",
      "epochs: 7/30.....  training_step: 14800....  training_loss: 1.140005.......  1.9220 sec/batch \n",
      "epochs: 7/30.....  training_step: 14850....  training_loss: 1.158953.......  3.5731 sec/batch \n",
      "epochs: 7/30.....  training_step: 14900....  training_loss: 1.194370.......  1.8691 sec/batch \n",
      "epochs: 7/30.....  training_step: 14950....  training_loss: 1.211526.......  1.9378 sec/batch \n",
      "epochs: 7/30.....  training_step: 15000....  training_loss: 1.227760.......  1.8988 sec/batch \n",
      "epochs: 7/30.....  training_step: 15050....  training_loss: 1.190419.......  1.9175 sec/batch \n",
      "epochs: 7/30.....  training_step: 15100....  training_loss: 1.172931.......  1.9155 sec/batch \n",
      "epochs: 7/30.....  training_step: 15150....  training_loss: 1.144458.......  1.8842 sec/batch \n",
      "epochs: 7/30.....  training_step: 15200....  training_loss: 1.148026.......  1.9230 sec/batch \n",
      "epochs: 7/30.....  training_step: 15250....  training_loss: 1.141697.......  1.8640 sec/batch \n",
      "epochs: 7/30.....  training_step: 15300....  training_loss: 1.147981.......  1.8801 sec/batch \n",
      "epochs: 7/30.....  training_step: 15350....  training_loss: 1.164087.......  1.8969 sec/batch \n",
      "epochs: 7/30.....  training_step: 15400....  training_loss: 1.175470.......  1.8876 sec/batch \n",
      "epochs: 7/30.....  training_step: 15450....  training_loss: 1.193388.......  1.9140 sec/batch \n",
      "epochs: 7/30.....  training_step: 15500....  training_loss: 1.178792.......  1.8905 sec/batch \n",
      "epochs: 7/30.....  training_step: 15550....  training_loss: 1.280375.......  1.9005 sec/batch \n",
      "epochs: 7/30.....  training_step: 15600....  training_loss: 1.202185.......  1.9285 sec/batch \n",
      "epochs: 7/30.....  training_step: 15650....  training_loss: 1.155962.......  1.8935 sec/batch \n",
      "epochs: 7/30.....  training_step: 15700....  training_loss: 1.259113.......  1.9415 sec/batch \n",
      "epochs: 7/30.....  training_step: 15750....  training_loss: 1.182299.......  1.9061 sec/batch \n",
      "epochs: 7/30.....  training_step: 15800....  training_loss: 1.122463.......  1.9188 sec/batch \n",
      "epochs: 7/30.....  training_step: 15850....  training_loss: 1.221149.......  1.8900 sec/batch \n",
      "epochs: 7/30.....  training_step: 15900....  training_loss: 1.244161.......  1.9158 sec/batch \n",
      "epochs: 7/30.....  training_step: 15950....  training_loss: 1.193891.......  1.9171 sec/batch \n",
      "epochs: 7/30.....  training_step: 16000....  training_loss: 1.143708.......  1.9131 sec/batch \n",
      "epochs: 7/30.....  training_step: 16050....  training_loss: 1.201508.......  1.9161 sec/batch \n",
      "epochs: 7/30.....  training_step: 16100....  training_loss: 1.163750.......  1.9071 sec/batch \n",
      "epochs: 7/30.....  training_step: 16150....  training_loss: 1.098817.......  1.9209 sec/batch \n",
      "epochs: 7/30.....  training_step: 16200....  training_loss: 1.120837.......  1.9059 sec/batch \n",
      "epochs: 7/30.....  training_step: 16250....  training_loss: 1.194282.......  1.8983 sec/batch \n",
      "epochs: 7/30.....  training_step: 16300....  training_loss: 1.133716.......  1.9386 sec/batch \n",
      "epochs: 7/30.....  training_step: 16350....  training_loss: 1.220052.......  1.9201 sec/batch \n",
      "epochs: 7/30.....  training_step: 16400....  training_loss: 1.171174.......  1.9374 sec/batch \n",
      "epochs: 7/30.....  training_step: 16450....  training_loss: 1.219092.......  1.9136 sec/batch \n",
      "epochs: 7/30.....  training_step: 16500....  training_loss: 1.165837.......  1.9200 sec/batch \n",
      "epochs: 7/30.....  training_step: 16550....  training_loss: 1.163453.......  1.9020 sec/batch \n",
      "epochs: 7/30.....  training_step: 16600....  training_loss: 1.200336.......  1.9113 sec/batch \n",
      "epochs: 7/30.....  training_step: 16650....  training_loss: 1.188663.......  1.9111 sec/batch \n",
      "epochs: 7/30.....  training_step: 16700....  training_loss: 1.259591.......  1.9338 sec/batch \n",
      "epochs: 7/30.....  training_step: 16750....  training_loss: 1.208291.......  1.9173 sec/batch \n",
      "epochs: 7/30.....  training_step: 16800....  training_loss: 1.128885.......  1.8988 sec/batch \n",
      "epochs: 7/30.....  training_step: 16850....  training_loss: 1.145634.......  1.8934 sec/batch \n",
      "epochs: 7/30.....  training_step: 16900....  training_loss: 1.131239.......  1.9082 sec/batch \n",
      "3776321\n",
      "epochs: 8/30.....  training_step: 16950....  training_loss: 1.155062.......  1.9313 sec/batch \n",
      "epochs: 8/30.....  training_step: 17000....  training_loss: 1.168728.......  1.8991 sec/batch \n",
      "epochs: 8/30.....  training_step: 17050....  training_loss: 1.096615.......  1.9244 sec/batch \n",
      "epochs: 8/30.....  training_step: 17100....  training_loss: 1.185948.......  1.9155 sec/batch \n",
      "epochs: 8/30.....  training_step: 17150....  training_loss: 1.135721.......  1.9061 sec/batch \n",
      "epochs: 8/30.....  training_step: 17200....  training_loss: 1.240058.......  1.9208 sec/batch \n",
      "epochs: 8/30.....  training_step: 17250....  training_loss: 1.196193.......  1.8968 sec/batch \n",
      "epochs: 8/30.....  training_step: 17300....  training_loss: 1.158157.......  1.9292 sec/batch \n",
      "epochs: 8/30.....  training_step: 17350....  training_loss: 1.212930.......  1.8905 sec/batch \n",
      "epochs: 8/30.....  training_step: 17400....  training_loss: 1.208663.......  1.9061 sec/batch \n",
      "epochs: 8/30.....  training_step: 17450....  training_loss: 1.123947.......  1.8810 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 8/30.....  training_step: 17500....  training_loss: 1.211040.......  1.9204 sec/batch \n",
      "epochs: 8/30.....  training_step: 17550....  training_loss: 1.231929.......  1.9240 sec/batch \n",
      "epochs: 8/30.....  training_step: 17600....  training_loss: 1.172602.......  1.8992 sec/batch \n",
      "epochs: 8/30.....  training_step: 17650....  training_loss: 1.151591.......  1.9003 sec/batch \n",
      "epochs: 8/30.....  training_step: 17700....  training_loss: 1.215925.......  1.9092 sec/batch \n",
      "epochs: 8/30.....  training_step: 17750....  training_loss: 1.165689.......  1.9073 sec/batch \n",
      "epochs: 8/30.....  training_step: 17800....  training_loss: 1.149063.......  1.9279 sec/batch \n",
      "epochs: 8/30.....  training_step: 17850....  training_loss: 1.151096.......  1.9082 sec/batch \n",
      "epochs: 8/30.....  training_step: 17900....  training_loss: 1.181073.......  1.9126 sec/batch \n",
      "epochs: 8/30.....  training_step: 17950....  training_loss: 1.135364.......  1.9047 sec/batch \n",
      "epochs: 8/30.....  training_step: 18000....  training_loss: 1.118041.......  1.9590 sec/batch \n",
      "epochs: 8/30.....  training_step: 18050....  training_loss: 1.114528.......  1.9645 sec/batch \n",
      "epochs: 8/30.....  training_step: 18100....  training_loss: 1.123513.......  1.9320 sec/batch \n",
      "epochs: 8/30.....  training_step: 18150....  training_loss: 1.157154.......  1.8881 sec/batch \n",
      "epochs: 8/30.....  training_step: 18200....  training_loss: 1.074261.......  1.9351 sec/batch \n",
      "epochs: 8/30.....  training_step: 18250....  training_loss: 1.133927.......  1.9277 sec/batch \n",
      "epochs: 8/30.....  training_step: 18300....  training_loss: 1.213433.......  1.9171 sec/batch \n",
      "epochs: 8/30.....  training_step: 18350....  training_loss: 1.136343.......  1.8864 sec/batch \n",
      "epochs: 8/30.....  training_step: 18400....  training_loss: 1.219666.......  1.9181 sec/batch \n",
      "epochs: 8/30.....  training_step: 18450....  training_loss: 1.142447.......  1.9505 sec/batch \n",
      "epochs: 8/30.....  training_step: 18500....  training_loss: 1.120823.......  1.9296 sec/batch \n",
      "epochs: 8/30.....  training_step: 18550....  training_loss: 1.193393.......  1.9267 sec/batch \n",
      "epochs: 8/30.....  training_step: 18600....  training_loss: 1.088805.......  1.9067 sec/batch \n",
      "epochs: 8/30.....  training_step: 18650....  training_loss: 1.210425.......  1.8612 sec/batch \n",
      "epochs: 8/30.....  training_step: 18700....  training_loss: 1.164267.......  1.9084 sec/batch \n",
      "epochs: 8/30.....  training_step: 18750....  training_loss: 1.193154.......  1.8897 sec/batch \n",
      "epochs: 8/30.....  training_step: 18800....  training_loss: 1.129266.......  1.9039 sec/batch \n",
      "epochs: 8/30.....  training_step: 18850....  training_loss: 1.131257.......  1.8972 sec/batch \n",
      "epochs: 8/30.....  training_step: 18900....  training_loss: 1.136681.......  1.8985 sec/batch \n",
      "epochs: 8/30.....  training_step: 18950....  training_loss: 1.158882.......  1.9298 sec/batch \n",
      "epochs: 8/30.....  training_step: 19000....  training_loss: 1.251117.......  1.8790 sec/batch \n",
      "epochs: 8/30.....  training_step: 19050....  training_loss: 1.135767.......  1.9021 sec/batch \n",
      "epochs: 8/30.....  training_step: 19100....  training_loss: 1.143704.......  1.8805 sec/batch \n",
      "epochs: 8/30.....  training_step: 19150....  training_loss: 1.110784.......  1.8834 sec/batch \n",
      "epochs: 8/30.....  training_step: 19200....  training_loss: 1.166305.......  1.8697 sec/batch \n",
      "epochs: 8/30.....  training_step: 19250....  training_loss: 1.179808.......  1.8814 sec/batch \n",
      "epochs: 8/30.....  training_step: 19300....  training_loss: 1.187212.......  1.9025 sec/batch \n",
      "3776321\n",
      "epochs: 9/30.....  training_step: 19350....  training_loss: 1.168200.......  1.9017 sec/batch \n",
      "epochs: 9/30.....  training_step: 19400....  training_loss: 1.190466.......  2.5126 sec/batch \n",
      "epochs: 9/30.....  training_step: 19450....  training_loss: 1.113765.......  1.9769 sec/batch \n",
      "epochs: 9/30.....  training_step: 19500....  training_loss: 1.179860.......  1.9919 sec/batch \n",
      "epochs: 9/30.....  training_step: 19550....  training_loss: 1.163168.......  2.0898 sec/batch \n",
      "epochs: 9/30.....  training_step: 19600....  training_loss: 1.140817.......  2.0608 sec/batch \n",
      "epochs: 9/30.....  training_step: 19650....  training_loss: 1.152533.......  1.9899 sec/batch \n",
      "epochs: 9/30.....  training_step: 19700....  training_loss: 1.134939.......  2.0108 sec/batch \n",
      "epochs: 9/30.....  training_step: 19750....  training_loss: 1.174208.......  2.0458 sec/batch \n",
      "epochs: 9/30.....  training_step: 19800....  training_loss: 1.194335.......  2.3577 sec/batch \n",
      "epochs: 9/30.....  training_step: 19850....  training_loss: 1.172195.......  2.5415 sec/batch \n",
      "epochs: 9/30.....  training_step: 19900....  training_loss: 1.121576.......  2.2987 sec/batch \n",
      "epochs: 9/30.....  training_step: 19950....  training_loss: 1.195420.......  2.0188 sec/batch \n",
      "epochs: 9/30.....  training_step: 20000....  training_loss: 1.195717.......  2.2837 sec/batch \n",
      "epochs: 9/30.....  training_step: 20050....  training_loss: 1.149222.......  2.0588 sec/batch \n",
      "epochs: 9/30.....  training_step: 20100....  training_loss: 1.163922.......  2.3337 sec/batch \n",
      "epochs: 9/30.....  training_step: 20150....  training_loss: 1.188409.......  2.1248 sec/batch \n",
      "epochs: 9/30.....  training_step: 20200....  training_loss: 1.187326.......  2.1967 sec/batch \n",
      "epochs: 9/30.....  training_step: 20250....  training_loss: 1.126056.......  2.1438 sec/batch \n",
      "epochs: 9/30.....  training_step: 20300....  training_loss: 1.206205.......  2.0658 sec/batch \n",
      "epochs: 9/30.....  training_step: 20350....  training_loss: 1.168543.......  2.1408 sec/batch \n",
      "epochs: 9/30.....  training_step: 20400....  training_loss: 1.150814.......  2.1158 sec/batch \n",
      "epochs: 9/30.....  training_step: 20450....  training_loss: 1.183196.......  2.1188 sec/batch \n",
      "epochs: 9/30.....  training_step: 20500....  training_loss: 1.204354.......  2.1288 sec/batch \n",
      "epochs: 9/30.....  training_step: 20550....  training_loss: 1.170459.......  2.0708 sec/batch \n",
      "epochs: 9/30.....  training_step: 20600....  training_loss: 1.153252.......  2.0288 sec/batch \n",
      "epochs: 9/30.....  training_step: 20650....  training_loss: 1.109736.......  2.0088 sec/batch \n",
      "epochs: 9/30.....  training_step: 20700....  training_loss: 1.110964.......  2.0568 sec/batch \n",
      "epochs: 9/30.....  training_step: 20750....  training_loss: 1.112876.......  3.1835 sec/batch \n",
      "epochs: 9/30.....  training_step: 20800....  training_loss: 1.164458.......  3.1990 sec/batch \n",
      "epochs: 9/30.....  training_step: 20850....  training_loss: 1.212840.......  4.8010 sec/batch \n",
      "epochs: 9/30.....  training_step: 20900....  training_loss: 1.197004.......  4.2279 sec/batch \n",
      "epochs: 9/30.....  training_step: 20950....  training_loss: 1.177532.......  1.8986 sec/batch \n",
      "epochs: 9/30.....  training_step: 21000....  training_loss: 1.073982.......  1.9010 sec/batch \n",
      "epochs: 9/30.....  training_step: 21050....  training_loss: 1.187824.......  4.7031 sec/batch \n",
      "epochs: 9/30.....  training_step: 21100....  training_loss: 1.142194.......  2.4416 sec/batch \n",
      "epochs: 9/30.....  training_step: 21150....  training_loss: 1.149682.......  2.3661 sec/batch \n",
      "epochs: 9/30.....  training_step: 21200....  training_loss: 1.215354.......  1.9222 sec/batch \n",
      "epochs: 9/30.....  training_step: 21250....  training_loss: 1.077435.......  2.7448 sec/batch \n",
      "epochs: 9/30.....  training_step: 21300....  training_loss: 1.114259.......  1.9107 sec/batch \n",
      "epochs: 9/30.....  training_step: 21350....  training_loss: 1.184960.......  3.7337 sec/batch \n",
      "epochs: 9/30.....  training_step: 21400....  training_loss: 1.213943.......  1.8885 sec/batch \n",
      "epochs: 9/30.....  training_step: 21450....  training_loss: 1.154320.......  1.9018 sec/batch \n",
      "epochs: 9/30.....  training_step: 21500....  training_loss: 1.158120.......  3.7264 sec/batch \n",
      "epochs: 9/30.....  training_step: 21550....  training_loss: 1.197459.......  1.8787 sec/batch \n",
      "epochs: 9/30.....  training_step: 21600....  training_loss: 1.204360.......  4.5894 sec/batch \n",
      "epochs: 9/30.....  training_step: 21650....  training_loss: 1.152634.......  1.8850 sec/batch \n",
      "epochs: 9/30.....  training_step: 21700....  training_loss: 1.154619.......  2.3485 sec/batch \n",
      "3776321\n",
      "epochs: 10/30.....  training_step: 21750....  training_loss: 1.196192.......  1.8788 sec/batch \n",
      "epochs: 10/30.....  training_step: 21800....  training_loss: 1.063371.......  1.9001 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 10/30.....  training_step: 21850....  training_loss: 1.142903.......  3.1759 sec/batch \n",
      "epochs: 10/30.....  training_step: 21900....  training_loss: 1.108879.......  1.8901 sec/batch \n",
      "epochs: 10/30.....  training_step: 21950....  training_loss: 1.071234.......  3.9364 sec/batch \n",
      "epochs: 10/30.....  training_step: 22000....  training_loss: 1.156960.......  4.7196 sec/batch \n",
      "epochs: 10/30.....  training_step: 22050....  training_loss: 1.097637.......  1.9415 sec/batch \n",
      "epochs: 10/30.....  training_step: 22100....  training_loss: 1.146177.......  1.8774 sec/batch \n",
      "epochs: 10/30.....  training_step: 22150....  training_loss: 1.164951.......  4.4863 sec/batch \n",
      "epochs: 10/30.....  training_step: 22200....  training_loss: 1.117164.......  1.8762 sec/batch \n",
      "epochs: 10/30.....  training_step: 22250....  training_loss: 1.142612.......  1.8857 sec/batch \n",
      "epochs: 10/30.....  training_step: 22300....  training_loss: 1.122859.......  1.9027 sec/batch \n",
      "epochs: 10/30.....  training_step: 22350....  training_loss: 1.119605.......  2.0835 sec/batch \n",
      "epochs: 10/30.....  training_step: 22400....  training_loss: 1.099808.......  1.8838 sec/batch \n",
      "epochs: 10/30.....  training_step: 22450....  training_loss: 1.177681.......  3.1541 sec/batch \n",
      "epochs: 10/30.....  training_step: 22500....  training_loss: 1.126864.......  1.8583 sec/batch \n",
      "epochs: 10/30.....  training_step: 22550....  training_loss: 1.126248.......  1.9041 sec/batch \n",
      "epochs: 10/30.....  training_step: 22600....  training_loss: 1.087475.......  2.0166 sec/batch \n",
      "epochs: 10/30.....  training_step: 22650....  training_loss: 1.172808.......  3.9426 sec/batch \n",
      "epochs: 10/30.....  training_step: 22700....  training_loss: 1.146300.......  4.6422 sec/batch \n",
      "epochs: 10/30.....  training_step: 22750....  training_loss: 1.141955.......  1.8982 sec/batch \n",
      "epochs: 10/30.....  training_step: 22800....  training_loss: 1.173161.......  4.0355 sec/batch \n",
      "epochs: 10/30.....  training_step: 22850....  training_loss: 1.150132.......  1.9387 sec/batch \n",
      "epochs: 10/30.....  training_step: 22900....  training_loss: 1.101667.......  2.2801 sec/batch \n",
      "epochs: 10/30.....  training_step: 22950....  training_loss: 1.118796.......  1.8961 sec/batch \n",
      "epochs: 10/30.....  training_step: 23000....  training_loss: 1.127650.......  2.3471 sec/batch \n",
      "epochs: 10/30.....  training_step: 23050....  training_loss: 1.136153.......  1.9525 sec/batch \n",
      "epochs: 10/30.....  training_step: 23100....  training_loss: 1.157950.......  2.1118 sec/batch \n",
      "epochs: 10/30.....  training_step: 23150....  training_loss: 1.086483.......  2.0177 sec/batch \n",
      "epochs: 10/30.....  training_step: 23200....  training_loss: 1.163983.......  1.9292 sec/batch \n",
      "epochs: 10/30.....  training_step: 23250....  training_loss: 1.065120.......  1.8916 sec/batch \n",
      "epochs: 10/30.....  training_step: 23300....  training_loss: 1.173838.......  1.9160 sec/batch \n",
      "epochs: 10/30.....  training_step: 23350....  training_loss: 1.141504.......  1.8987 sec/batch \n",
      "epochs: 10/30.....  training_step: 23400....  training_loss: 1.230343.......  1.9355 sec/batch \n",
      "epochs: 10/30.....  training_step: 23450....  training_loss: 1.110834.......  1.9078 sec/batch \n",
      "epochs: 10/30.....  training_step: 23500....  training_loss: 1.124336.......  1.9065 sec/batch \n",
      "epochs: 10/30.....  training_step: 23550....  training_loss: 1.175584.......  1.9660 sec/batch \n",
      "epochs: 10/30.....  training_step: 23600....  training_loss: 1.124489.......  1.9073 sec/batch \n",
      "epochs: 10/30.....  training_step: 23650....  training_loss: 1.124874.......  1.9057 sec/batch \n",
      "epochs: 10/30.....  training_step: 23700....  training_loss: 1.207651.......  1.9061 sec/batch \n",
      "epochs: 10/30.....  training_step: 23750....  training_loss: 1.241446.......  1.8991 sec/batch \n",
      "epochs: 10/30.....  training_step: 23800....  training_loss: 1.155395.......  1.9420 sec/batch \n",
      "epochs: 10/30.....  training_step: 23850....  training_loss: 1.207219.......  1.9036 sec/batch \n",
      "epochs: 10/30.....  training_step: 23900....  training_loss: 1.212815.......  1.8966 sec/batch \n",
      "epochs: 10/30.....  training_step: 23950....  training_loss: 1.108849.......  1.9166 sec/batch \n",
      "epochs: 10/30.....  training_step: 24000....  training_loss: 1.190382.......  1.9031 sec/batch \n",
      "epochs: 10/30.....  training_step: 24050....  training_loss: 1.170922.......  1.9102 sec/batch \n",
      "epochs: 10/30.....  training_step: 24100....  training_loss: 1.173841.......  1.8957 sec/batch \n",
      "epochs: 10/30.....  training_step: 24150....  training_loss: 1.477170.......  1.8891 sec/batch \n",
      "3776321\n",
      "epochs: 11/30.....  training_step: 24200....  training_loss: 1.104230.......  1.8955 sec/batch \n",
      "epochs: 11/30.....  training_step: 24250....  training_loss: 1.131435.......  1.8927 sec/batch \n",
      "epochs: 11/30.....  training_step: 24300....  training_loss: 1.161710.......  1.9214 sec/batch \n",
      "epochs: 11/30.....  training_step: 24350....  training_loss: 1.180069.......  1.9251 sec/batch \n",
      "epochs: 11/30.....  training_step: 24400....  training_loss: 1.148218.......  1.9392 sec/batch \n",
      "epochs: 11/30.....  training_step: 24450....  training_loss: 1.113331.......  1.9132 sec/batch \n",
      "epochs: 11/30.....  training_step: 24500....  training_loss: 1.145135.......  1.9137 sec/batch \n",
      "epochs: 11/30.....  training_step: 24550....  training_loss: 1.094294.......  1.8829 sec/batch \n",
      "epochs: 11/30.....  training_step: 24600....  training_loss: 1.163636.......  1.9074 sec/batch \n",
      "epochs: 11/30.....  training_step: 24650....  training_loss: 1.165207.......  1.8850 sec/batch \n",
      "epochs: 11/30.....  training_step: 24700....  training_loss: 1.066163.......  1.8924 sec/batch \n",
      "epochs: 11/30.....  training_step: 24750....  training_loss: 1.162138.......  1.8780 sec/batch \n",
      "epochs: 11/30.....  training_step: 24800....  training_loss: 1.164046.......  1.9362 sec/batch \n",
      "epochs: 11/30.....  training_step: 24850....  training_loss: 1.120023.......  1.8806 sec/batch \n",
      "epochs: 11/30.....  training_step: 24900....  training_loss: 1.123158.......  1.9031 sec/batch \n",
      "epochs: 11/30.....  training_step: 24950....  training_loss: 1.129336.......  1.8927 sec/batch \n",
      "epochs: 11/30.....  training_step: 25000....  training_loss: 1.057020.......  1.8970 sec/batch \n",
      "epochs: 11/30.....  training_step: 25050....  training_loss: 1.085454.......  1.8970 sec/batch \n",
      "epochs: 11/30.....  training_step: 25100....  training_loss: 1.191561.......  1.8798 sec/batch \n",
      "epochs: 11/30.....  training_step: 25150....  training_loss: 1.125492.......  1.9134 sec/batch \n",
      "epochs: 11/30.....  training_step: 25200....  training_loss: 1.134805.......  1.9048 sec/batch \n",
      "epochs: 11/30.....  training_step: 25250....  training_loss: 1.118129.......  1.8907 sec/batch \n",
      "epochs: 11/30.....  training_step: 25300....  training_loss: 1.187791.......  1.8905 sec/batch \n",
      "epochs: 11/30.....  training_step: 25350....  training_loss: 1.144010.......  1.8906 sec/batch \n",
      "epochs: 11/30.....  training_step: 25400....  training_loss: 1.145910.......  1.9096 sec/batch \n",
      "epochs: 11/30.....  training_step: 25450....  training_loss: 1.155640.......  1.8760 sec/batch \n",
      "epochs: 11/30.....  training_step: 25500....  training_loss: 1.121750.......  1.8914 sec/batch \n",
      "epochs: 11/30.....  training_step: 25550....  training_loss: 1.171296.......  1.8782 sec/batch \n",
      "epochs: 11/30.....  training_step: 25600....  training_loss: 1.117419.......  1.8731 sec/batch \n",
      "epochs: 11/30.....  training_step: 25650....  training_loss: 1.104153.......  1.9132 sec/batch \n",
      "epochs: 11/30.....  training_step: 25700....  training_loss: 1.041988.......  1.9153 sec/batch \n",
      "epochs: 11/30.....  training_step: 25750....  training_loss: 1.067853.......  1.9112 sec/batch \n",
      "epochs: 11/30.....  training_step: 25800....  training_loss: 1.077935.......  1.8908 sec/batch \n",
      "epochs: 11/30.....  training_step: 25850....  training_loss: 1.243557.......  1.9036 sec/batch \n",
      "epochs: 11/30.....  training_step: 25900....  training_loss: 1.143275.......  1.9061 sec/batch \n",
      "epochs: 11/30.....  training_step: 25950....  training_loss: 1.094689.......  2.0841 sec/batch \n",
      "epochs: 11/30.....  training_step: 26000....  training_loss: 1.154469.......  1.9055 sec/batch \n",
      "epochs: 11/30.....  training_step: 26050....  training_loss: 1.164333.......  1.9311 sec/batch \n",
      "epochs: 11/30.....  training_step: 26100....  training_loss: 1.192716.......  1.9332 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 11/30.....  training_step: 26150....  training_loss: 1.199814.......  1.9717 sec/batch \n",
      "epochs: 11/30.....  training_step: 26200....  training_loss: 1.081126.......  1.9362 sec/batch \n",
      "epochs: 11/30.....  training_step: 26250....  training_loss: 1.167375.......  2.9023 sec/batch \n",
      "epochs: 11/30.....  training_step: 26300....  training_loss: 1.139787.......  2.3497 sec/batch \n",
      "epochs: 11/30.....  training_step: 26350....  training_loss: 1.161714.......  1.9347 sec/batch \n",
      "epochs: 11/30.....  training_step: 26400....  training_loss: 1.110473.......  1.9486 sec/batch \n",
      "epochs: 11/30.....  training_step: 26450....  training_loss: 1.054537.......  1.9379 sec/batch \n",
      "epochs: 11/30.....  training_step: 26500....  training_loss: 1.063625.......  2.0157 sec/batch \n",
      "epochs: 11/30.....  training_step: 26550....  training_loss: 1.133103.......  1.9391 sec/batch \n",
      "3776321\n",
      "epochs: 12/30.....  training_step: 26600....  training_loss: 1.119194.......  1.9281 sec/batch \n",
      "epochs: 12/30.....  training_step: 26650....  training_loss: 1.101718.......  1.9336 sec/batch \n",
      "epochs: 12/30.....  training_step: 26700....  training_loss: 1.162825.......  1.9505 sec/batch \n",
      "epochs: 12/30.....  training_step: 26750....  training_loss: 1.156149.......  1.9076 sec/batch \n",
      "epochs: 12/30.....  training_step: 26800....  training_loss: 1.168676.......  1.9098 sec/batch \n",
      "epochs: 12/30.....  training_step: 26850....  training_loss: 1.212157.......  1.9021 sec/batch \n",
      "epochs: 12/30.....  training_step: 26900....  training_loss: 1.163632.......  1.9159 sec/batch \n",
      "epochs: 12/30.....  training_step: 26950....  training_loss: 1.067041.......  1.8830 sec/batch \n",
      "epochs: 12/30.....  training_step: 27000....  training_loss: 1.121862.......  1.9018 sec/batch \n",
      "epochs: 12/30.....  training_step: 27050....  training_loss: 1.081733.......  1.8918 sec/batch \n",
      "epochs: 12/30.....  training_step: 27100....  training_loss: 1.120891.......  1.9264 sec/batch \n",
      "epochs: 12/30.....  training_step: 27150....  training_loss: 1.057632.......  1.9281 sec/batch \n",
      "epochs: 12/30.....  training_step: 27200....  training_loss: 1.158628.......  1.8868 sec/batch \n",
      "epochs: 12/30.....  training_step: 27250....  training_loss: 1.079742.......  2.4036 sec/batch \n",
      "epochs: 12/30.....  training_step: 27300....  training_loss: 1.036351.......  2.1367 sec/batch \n",
      "epochs: 12/30.....  training_step: 27350....  training_loss: 1.119600.......  2.1299 sec/batch \n",
      "epochs: 12/30.....  training_step: 27400....  training_loss: 1.114785.......  1.8804 sec/batch \n",
      "epochs: 12/30.....  training_step: 27450....  training_loss: 1.125314.......  1.8959 sec/batch \n",
      "epochs: 12/30.....  training_step: 27500....  training_loss: 1.101257.......  1.8818 sec/batch \n",
      "epochs: 12/30.....  training_step: 27550....  training_loss: 1.063383.......  1.9017 sec/batch \n",
      "epochs: 12/30.....  training_step: 27600....  training_loss: 1.140234.......  1.8762 sec/batch \n",
      "epochs: 12/30.....  training_step: 27650....  training_loss: 1.092427.......  1.8941 sec/batch \n",
      "epochs: 12/30.....  training_step: 27700....  training_loss: 1.092176.......  1.9257 sec/batch \n",
      "epochs: 12/30.....  training_step: 27750....  training_loss: 1.197812.......  1.8851 sec/batch \n",
      "epochs: 12/30.....  training_step: 27800....  training_loss: 1.100338.......  1.8680 sec/batch \n",
      "epochs: 12/30.....  training_step: 27850....  training_loss: 1.125894.......  1.8781 sec/batch \n",
      "epochs: 12/30.....  training_step: 27900....  training_loss: 1.065580.......  1.9245 sec/batch \n",
      "epochs: 12/30.....  training_step: 27950....  training_loss: 1.152557.......  1.8725 sec/batch \n",
      "epochs: 12/30.....  training_step: 28000....  training_loss: 1.102345.......  1.8681 sec/batch \n",
      "epochs: 12/30.....  training_step: 28050....  training_loss: 1.104170.......  1.8671 sec/batch \n",
      "epochs: 12/30.....  training_step: 28100....  training_loss: 1.156395.......  1.8823 sec/batch \n",
      "epochs: 12/30.....  training_step: 28150....  training_loss: 1.143647.......  1.9107 sec/batch \n",
      "epochs: 12/30.....  training_step: 28200....  training_loss: 1.092928.......  1.8676 sec/batch \n",
      "epochs: 12/30.....  training_step: 28250....  training_loss: 1.063108.......  1.8708 sec/batch \n",
      "epochs: 12/30.....  training_step: 28300....  training_loss: 1.075699.......  1.8779 sec/batch \n",
      "epochs: 12/30.....  training_step: 28350....  training_loss: 1.138745.......  1.8791 sec/batch \n",
      "epochs: 12/30.....  training_step: 28400....  training_loss: 1.097613.......  1.8922 sec/batch \n",
      "epochs: 12/30.....  training_step: 28450....  training_loss: 1.156907.......  1.8992 sec/batch \n",
      "epochs: 12/30.....  training_step: 28500....  training_loss: 1.110388.......  1.8748 sec/batch \n",
      "epochs: 12/30.....  training_step: 28550....  training_loss: 1.169275.......  1.8991 sec/batch \n",
      "epochs: 12/30.....  training_step: 28600....  training_loss: 1.073939.......  1.8648 sec/batch \n",
      "epochs: 12/30.....  training_step: 28650....  training_loss: 1.159913.......  1.9116 sec/batch \n",
      "epochs: 12/30.....  training_step: 28700....  training_loss: 1.144277.......  1.8809 sec/batch \n",
      "epochs: 12/30.....  training_step: 28750....  training_loss: 1.113326.......  1.8716 sec/batch \n",
      "epochs: 12/30.....  training_step: 28800....  training_loss: 1.120816.......  3.4994 sec/batch \n",
      "epochs: 12/30.....  training_step: 28850....  training_loss: 1.062976.......  3.1175 sec/batch \n",
      "epochs: 12/30.....  training_step: 28900....  training_loss: 1.103790.......  1.9217 sec/batch \n",
      "epochs: 12/30.....  training_step: 28950....  training_loss: 1.123862.......  4.7048 sec/batch \n",
      "3776321\n",
      "epochs: 13/30.....  training_step: 29000....  training_loss: 1.178212.......  1.8709 sec/batch \n",
      "epochs: 13/30.....  training_step: 29050....  training_loss: 1.108339.......  1.9071 sec/batch \n",
      "epochs: 13/30.....  training_step: 29100....  training_loss: 1.131218.......  2.0195 sec/batch \n",
      "epochs: 13/30.....  training_step: 29150....  training_loss: 1.102113.......  1.9074 sec/batch \n",
      "epochs: 13/30.....  training_step: 29200....  training_loss: 1.149962.......  2.0608 sec/batch \n",
      "epochs: 13/30.....  training_step: 29250....  training_loss: 1.137816.......  1.9235 sec/batch \n",
      "epochs: 13/30.....  training_step: 29300....  training_loss: 1.101712.......  1.8742 sec/batch \n",
      "epochs: 13/30.....  training_step: 29350....  training_loss: 1.127101.......  4.6651 sec/batch \n",
      "epochs: 13/30.....  training_step: 29400....  training_loss: 1.199422.......  1.8846 sec/batch \n",
      "epochs: 13/30.....  training_step: 29450....  training_loss: 1.168512.......  3.7615 sec/batch \n",
      "epochs: 13/30.....  training_step: 29500....  training_loss: 1.169765.......  1.9201 sec/batch \n",
      "epochs: 13/30.....  training_step: 29550....  training_loss: 1.085523.......  1.8934 sec/batch \n",
      "epochs: 13/30.....  training_step: 29600....  training_loss: 1.097756.......  3.8703 sec/batch \n",
      "epochs: 13/30.....  training_step: 29650....  training_loss: 1.180523.......  1.9040 sec/batch \n",
      "epochs: 13/30.....  training_step: 29700....  training_loss: 1.177827.......  3.1975 sec/batch \n",
      "epochs: 13/30.....  training_step: 29750....  training_loss: 1.092664.......  3.0997 sec/batch \n",
      "epochs: 13/30.....  training_step: 29800....  training_loss: 1.123230.......  1.8907 sec/batch \n",
      "epochs: 13/30.....  training_step: 29850....  training_loss: 1.086763.......  1.9168 sec/batch \n",
      "epochs: 13/30.....  training_step: 29900....  training_loss: 1.160779.......  4.6109 sec/batch \n",
      "epochs: 13/30.....  training_step: 29950....  training_loss: 1.050158.......  1.8839 sec/batch \n",
      "epochs: 13/30.....  training_step: 30000....  training_loss: 1.054390.......  1.8880 sec/batch \n",
      "epochs: 13/30.....  training_step: 30050....  training_loss: 1.107858.......  3.2772 sec/batch \n",
      "epochs: 13/30.....  training_step: 30100....  training_loss: 1.204092.......  1.9061 sec/batch \n",
      "epochs: 13/30.....  training_step: 30150....  training_loss: 1.155518.......  1.8909 sec/batch \n",
      "epochs: 13/30.....  training_step: 30200....  training_loss: 1.160472.......  4.1750 sec/batch \n",
      "epochs: 13/30.....  training_step: 30250....  training_loss: 1.093200.......  2.1659 sec/batch \n",
      "epochs: 13/30.....  training_step: 30300....  training_loss: 1.105926.......  1.9157 sec/batch \n",
      "epochs: 13/30.....  training_step: 30350....  training_loss: 1.152546.......  3.5448 sec/batch \n",
      "epochs: 13/30.....  training_step: 30400....  training_loss: 1.077140.......  1.9002 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 13/30.....  training_step: 30450....  training_loss: 1.149352.......  1.8907 sec/batch \n",
      "epochs: 13/30.....  training_step: 30500....  training_loss: 1.029563.......  3.9701 sec/batch \n",
      "epochs: 13/30.....  training_step: 30550....  training_loss: 1.108748.......  1.8841 sec/batch \n",
      "epochs: 13/30.....  training_step: 30600....  training_loss: 1.053076.......  4.7656 sec/batch \n",
      "epochs: 13/30.....  training_step: 30650....  training_loss: 1.182275.......  1.9206 sec/batch \n",
      "epochs: 13/30.....  training_step: 30700....  training_loss: 1.121085.......  1.9793 sec/batch \n",
      "epochs: 13/30.....  training_step: 30750....  training_loss: 1.142816.......  1.8821 sec/batch \n",
      "epochs: 13/30.....  training_step: 30800....  training_loss: 1.136583.......  1.8872 sec/batch \n",
      "epochs: 13/30.....  training_step: 30850....  training_loss: 1.134735.......  1.9016 sec/batch \n",
      "epochs: 13/30.....  training_step: 30900....  training_loss: 1.040102.......  1.8788 sec/batch \n",
      "epochs: 13/30.....  training_step: 30950....  training_loss: 1.152655.......  4.2619 sec/batch \n",
      "epochs: 13/30.....  training_step: 31000....  training_loss: 1.124073.......  1.8931 sec/batch \n",
      "epochs: 13/30.....  training_step: 31050....  training_loss: 1.098772.......  1.8811 sec/batch \n",
      "epochs: 13/30.....  training_step: 31100....  training_loss: 1.129569.......  4.5080 sec/batch \n",
      "epochs: 13/30.....  training_step: 31150....  training_loss: 1.124491.......  1.8957 sec/batch \n",
      "epochs: 13/30.....  training_step: 31200....  training_loss: 1.138617.......  2.3113 sec/batch \n",
      "epochs: 13/30.....  training_step: 31250....  training_loss: 1.155952.......  1.8839 sec/batch \n",
      "epochs: 13/30.....  training_step: 31300....  training_loss: 1.197669.......  1.8781 sec/batch \n",
      "epochs: 13/30.....  training_step: 31350....  training_loss: 1.181976.......  1.9013 sec/batch \n",
      "3776321\n",
      "epochs: 14/30.....  training_step: 31400....  training_loss: 1.090343.......  1.8947 sec/batch \n",
      "epochs: 14/30.....  training_step: 31450....  training_loss: 1.105108.......  1.8671 sec/batch \n",
      "epochs: 14/30.....  training_step: 31500....  training_loss: 1.175027.......  1.9069 sec/batch \n",
      "epochs: 14/30.....  training_step: 31550....  training_loss: 1.077245.......  1.9083 sec/batch \n",
      "epochs: 14/30.....  training_step: 31600....  training_loss: 1.144907.......  1.9013 sec/batch \n",
      "epochs: 14/30.....  training_step: 31650....  training_loss: 1.135314.......  1.9051 sec/batch \n",
      "epochs: 14/30.....  training_step: 31700....  training_loss: 1.133849.......  1.9061 sec/batch \n",
      "epochs: 14/30.....  training_step: 31750....  training_loss: 1.131781.......  1.9059 sec/batch \n",
      "epochs: 14/30.....  training_step: 31800....  training_loss: 1.118547.......  1.9593 sec/batch \n",
      "epochs: 14/30.....  training_step: 31850....  training_loss: 1.102685.......  1.9216 sec/batch \n",
      "epochs: 14/30.....  training_step: 31900....  training_loss: 1.138368.......  1.9386 sec/batch \n",
      "epochs: 14/30.....  training_step: 31950....  training_loss: 1.134771.......  1.9359 sec/batch \n",
      "epochs: 14/30.....  training_step: 32000....  training_loss: 1.109884.......  1.9319 sec/batch \n",
      "epochs: 14/30.....  training_step: 32050....  training_loss: 1.150148.......  1.9772 sec/batch \n",
      "epochs: 14/30.....  training_step: 32100....  training_loss: 1.130755.......  1.9461 sec/batch \n",
      "epochs: 14/30.....  training_step: 32150....  training_loss: 1.090874.......  1.9419 sec/batch \n",
      "epochs: 14/30.....  training_step: 32200....  training_loss: 1.138494.......  1.9182 sec/batch \n",
      "epochs: 14/30.....  training_step: 32250....  training_loss: 1.036518.......  1.9306 sec/batch \n",
      "epochs: 14/30.....  training_step: 32300....  training_loss: 1.121310.......  1.9536 sec/batch \n",
      "epochs: 14/30.....  training_step: 32350....  training_loss: 1.149241.......  1.9478 sec/batch \n",
      "epochs: 14/30.....  training_step: 32400....  training_loss: 1.083391.......  1.9036 sec/batch \n",
      "epochs: 14/30.....  training_step: 32450....  training_loss: 1.140995.......  1.9390 sec/batch \n",
      "epochs: 14/30.....  training_step: 32500....  training_loss: 1.131618.......  1.9239 sec/batch \n",
      "epochs: 14/30.....  training_step: 32550....  training_loss: 1.101150.......  1.9253 sec/batch \n",
      "epochs: 14/30.....  training_step: 32600....  training_loss: 1.093792.......  2.1815 sec/batch \n",
      "epochs: 14/30.....  training_step: 32650....  training_loss: 1.065596.......  1.9276 sec/batch \n",
      "epochs: 14/30.....  training_step: 32700....  training_loss: 1.075394.......  1.9204 sec/batch \n",
      "epochs: 14/30.....  training_step: 32750....  training_loss: 1.091379.......  1.9222 sec/batch \n",
      "epochs: 14/30.....  training_step: 32800....  training_loss: 1.153203.......  1.9500 sec/batch \n",
      "epochs: 14/30.....  training_step: 32850....  training_loss: 1.074862.......  1.9069 sec/batch \n",
      "epochs: 14/30.....  training_step: 32900....  training_loss: 1.110991.......  1.9152 sec/batch \n",
      "epochs: 14/30.....  training_step: 32950....  training_loss: 1.074025.......  1.9155 sec/batch \n",
      "epochs: 14/30.....  training_step: 33000....  training_loss: 1.045045.......  1.8763 sec/batch \n",
      "epochs: 14/30.....  training_step: 33050....  training_loss: 1.131427.......  1.9323 sec/batch \n",
      "epochs: 14/30.....  training_step: 33100....  training_loss: 1.146624.......  1.9025 sec/batch \n",
      "epochs: 14/30.....  training_step: 33150....  training_loss: 1.135613.......  1.9087 sec/batch \n",
      "epochs: 14/30.....  training_step: 33200....  training_loss: 1.108094.......  1.9191 sec/batch \n",
      "epochs: 14/30.....  training_step: 33250....  training_loss: 1.127688.......  1.9003 sec/batch \n",
      "epochs: 14/30.....  training_step: 33300....  training_loss: 1.105598.......  2.1371 sec/batch \n",
      "epochs: 14/30.....  training_step: 33350....  training_loss: 1.077465.......  1.9190 sec/batch \n",
      "epochs: 14/30.....  training_step: 33400....  training_loss: 1.154649.......  1.8788 sec/batch \n",
      "epochs: 14/30.....  training_step: 33450....  training_loss: 1.121317.......  1.9045 sec/batch \n",
      "epochs: 14/30.....  training_step: 33500....  training_loss: 1.170472.......  1.9609 sec/batch \n",
      "epochs: 14/30.....  training_step: 33550....  training_loss: 1.106173.......  2.0348 sec/batch \n",
      "epochs: 14/30.....  training_step: 33600....  training_loss: 1.142053.......  1.9252 sec/batch \n",
      "epochs: 14/30.....  training_step: 33650....  training_loss: 1.174437.......  1.9410 sec/batch \n",
      "epochs: 14/30.....  training_step: 33700....  training_loss: 1.053411.......  1.9345 sec/batch \n",
      "epochs: 14/30.....  training_step: 33750....  training_loss: 1.142476.......  1.9209 sec/batch \n",
      "epochs: 14/30.....  training_step: 33800....  training_loss: 1.052553.......  1.9498 sec/batch \n",
      "3776321\n",
      "epochs: 15/30.....  training_step: 33850....  training_loss: 1.040136.......  1.9251 sec/batch \n",
      "epochs: 15/30.....  training_step: 33900....  training_loss: 1.088250.......  1.9458 sec/batch \n",
      "epochs: 15/30.....  training_step: 33950....  training_loss: 1.053826.......  1.9217 sec/batch \n",
      "epochs: 15/30.....  training_step: 34000....  training_loss: 1.149986.......  1.9386 sec/batch \n",
      "epochs: 15/30.....  training_step: 34050....  training_loss: 1.174969.......  2.1433 sec/batch \n",
      "epochs: 15/30.....  training_step: 34100....  training_loss: 1.155860.......  2.7297 sec/batch \n",
      "epochs: 15/30.....  training_step: 34150....  training_loss: 1.218893.......  2.2599 sec/batch \n",
      "epochs: 15/30.....  training_step: 34200....  training_loss: 1.102821.......  2.4107 sec/batch \n",
      "epochs: 15/30.....  training_step: 34250....  training_loss: 1.136588.......  1.9789 sec/batch \n",
      "epochs: 15/30.....  training_step: 34300....  training_loss: 1.157823.......  2.0271 sec/batch \n",
      "epochs: 15/30.....  training_step: 34350....  training_loss: 1.130091.......  1.9978 sec/batch \n",
      "epochs: 15/30.....  training_step: 34400....  training_loss: 1.051052.......  2.2783 sec/batch \n",
      "epochs: 15/30.....  training_step: 34450....  training_loss: 1.112107.......  1.9603 sec/batch \n",
      "epochs: 15/30.....  training_step: 34500....  training_loss: 1.132994.......  1.9993 sec/batch \n",
      "epochs: 15/30.....  training_step: 34550....  training_loss: 1.043387.......  1.9906 sec/batch \n",
      "epochs: 15/30.....  training_step: 34600....  training_loss: 1.101845.......  1.9861 sec/batch \n",
      "epochs: 15/30.....  training_step: 34650....  training_loss: 0.997853.......  2.0145 sec/batch \n",
      "epochs: 15/30.....  training_step: 34700....  training_loss: 1.098386.......  1.9742 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 15/30.....  training_step: 34750....  training_loss: 1.064943.......  1.9793 sec/batch \n",
      "epochs: 15/30.....  training_step: 34800....  training_loss: 1.075418.......  1.9913 sec/batch \n",
      "epochs: 15/30.....  training_step: 34850....  training_loss: 1.074800.......  1.9820 sec/batch \n",
      "epochs: 15/30.....  training_step: 34900....  training_loss: 1.130775.......  1.9965 sec/batch \n",
      "epochs: 15/30.....  training_step: 34950....  training_loss: 1.081770.......  2.0091 sec/batch \n",
      "epochs: 15/30.....  training_step: 35000....  training_loss: 1.084975.......  2.2063 sec/batch \n",
      "epochs: 15/30.....  training_step: 35050....  training_loss: 1.113509.......  2.0077 sec/batch \n",
      "epochs: 15/30.....  training_step: 35100....  training_loss: 1.051835.......  2.0218 sec/batch \n",
      "epochs: 15/30.....  training_step: 35150....  training_loss: 1.188031.......  2.0150 sec/batch \n",
      "epochs: 15/30.....  training_step: 35200....  training_loss: 1.089883.......  1.9822 sec/batch \n",
      "epochs: 15/30.....  training_step: 35250....  training_loss: 1.083971.......  2.0684 sec/batch \n",
      "epochs: 15/30.....  training_step: 35300....  training_loss: 1.098107.......  2.2176 sec/batch \n",
      "epochs: 15/30.....  training_step: 35350....  training_loss: 1.084856.......  2.0041 sec/batch \n",
      "epochs: 15/30.....  training_step: 35400....  training_loss: 1.157088.......  1.9925 sec/batch \n",
      "epochs: 15/30.....  training_step: 35450....  training_loss: 1.162042.......  1.9946 sec/batch \n",
      "epochs: 15/30.....  training_step: 35500....  training_loss: 1.099920.......  2.0570 sec/batch \n",
      "epochs: 15/30.....  training_step: 35550....  training_loss: 1.144367.......  2.2013 sec/batch \n",
      "epochs: 15/30.....  training_step: 35600....  training_loss: 1.112147.......  2.0838 sec/batch \n",
      "epochs: 15/30.....  training_step: 35650....  training_loss: 1.139231.......  2.0549 sec/batch \n",
      "epochs: 15/30.....  training_step: 35700....  training_loss: 1.091253.......  2.0088 sec/batch \n",
      "epochs: 15/30.....  training_step: 35750....  training_loss: 1.122973.......  1.9470 sec/batch \n",
      "epochs: 15/30.....  training_step: 35800....  training_loss: 1.077364.......  2.5448 sec/batch \n",
      "epochs: 15/30.....  training_step: 35850....  training_loss: 1.129991.......  1.9754 sec/batch \n",
      "epochs: 15/30.....  training_step: 35900....  training_loss: 1.183548.......  1.9673 sec/batch \n",
      "epochs: 15/30.....  training_step: 35950....  training_loss: 1.144058.......  2.0476 sec/batch \n",
      "epochs: 15/30.....  training_step: 36000....  training_loss: 1.112279.......  2.0310 sec/batch \n",
      "epochs: 15/30.....  training_step: 36050....  training_loss: 1.068936.......  1.9446 sec/batch \n",
      "epochs: 15/30.....  training_step: 36100....  training_loss: 1.203312.......  1.9782 sec/batch \n",
      "epochs: 15/30.....  training_step: 36150....  training_loss: 1.072085.......  2.0034 sec/batch \n",
      "epochs: 15/30.....  training_step: 36200....  training_loss: 1.138338.......  2.0073 sec/batch \n",
      "3776321\n",
      "epochs: 16/30.....  training_step: 36250....  training_loss: 1.126120.......  1.9495 sec/batch \n",
      "epochs: 16/30.....  training_step: 36300....  training_loss: 1.096318.......  2.3251 sec/batch \n",
      "epochs: 16/30.....  training_step: 36350....  training_loss: 1.071918.......  2.0506 sec/batch \n",
      "epochs: 16/30.....  training_step: 36400....  training_loss: 1.162074.......  1.9461 sec/batch \n",
      "epochs: 16/30.....  training_step: 36450....  training_loss: 1.135779.......  1.9669 sec/batch \n",
      "epochs: 16/30.....  training_step: 36500....  training_loss: 1.103700.......  1.9686 sec/batch \n",
      "epochs: 16/30.....  training_step: 36550....  training_loss: 1.083883.......  2.9698 sec/batch \n",
      "epochs: 16/30.....  training_step: 36600....  training_loss: 1.185178.......  1.9786 sec/batch \n",
      "epochs: 16/30.....  training_step: 36650....  training_loss: 1.099471.......  1.9686 sec/batch \n",
      "epochs: 16/30.....  training_step: 36700....  training_loss: 1.159166.......  1.9732 sec/batch \n",
      "epochs: 16/30.....  training_step: 36750....  training_loss: 1.220190.......  1.9720 sec/batch \n",
      "epochs: 16/30.....  training_step: 36800....  training_loss: 1.093893.......  1.9530 sec/batch \n",
      "epochs: 16/30.....  training_step: 36850....  training_loss: 1.124469.......  2.0136 sec/batch \n",
      "epochs: 16/30.....  training_step: 36900....  training_loss: 1.025678.......  2.0078 sec/batch \n",
      "epochs: 16/30.....  training_step: 36950....  training_loss: 1.170284.......  1.9999 sec/batch \n",
      "epochs: 16/30.....  training_step: 37000....  training_loss: 1.041649.......  1.9486 sec/batch \n",
      "epochs: 16/30.....  training_step: 37050....  training_loss: 1.109192.......  1.9597 sec/batch \n",
      "epochs: 16/30.....  training_step: 37100....  training_loss: 1.130145.......  1.9951 sec/batch \n",
      "epochs: 16/30.....  training_step: 37150....  training_loss: 1.051866.......  1.9599 sec/batch \n",
      "epochs: 16/30.....  training_step: 37200....  training_loss: 1.109297.......  2.0288 sec/batch \n",
      "epochs: 16/30.....  training_step: 37250....  training_loss: 1.127728.......  1.9945 sec/batch \n",
      "epochs: 16/30.....  training_step: 37300....  training_loss: 1.105801.......  1.9530 sec/batch \n",
      "epochs: 16/30.....  training_step: 37350....  training_loss: 1.120016.......  1.9996 sec/batch \n",
      "epochs: 16/30.....  training_step: 37400....  training_loss: 1.094612.......  1.9375 sec/batch \n",
      "epochs: 16/30.....  training_step: 37450....  training_loss: 1.105313.......  1.9537 sec/batch \n",
      "epochs: 16/30.....  training_step: 37500....  training_loss: 1.095428.......  1.9774 sec/batch \n",
      "epochs: 16/30.....  training_step: 37550....  training_loss: 1.152470.......  2.0028 sec/batch \n",
      "epochs: 16/30.....  training_step: 37600....  training_loss: 1.077875.......  2.0051 sec/batch \n",
      "epochs: 16/30.....  training_step: 37650....  training_loss: 1.122633.......  2.2678 sec/batch \n",
      "epochs: 16/30.....  training_step: 37700....  training_loss: 1.092121.......  2.1199 sec/batch \n",
      "epochs: 16/30.....  training_step: 37750....  training_loss: 1.155406.......  1.9701 sec/batch \n",
      "epochs: 16/30.....  training_step: 37800....  training_loss: 1.185570.......  2.0110 sec/batch \n",
      "epochs: 16/30.....  training_step: 37850....  training_loss: 1.040235.......  2.2078 sec/batch \n",
      "epochs: 16/30.....  training_step: 37900....  training_loss: 1.044170.......  2.0164 sec/batch \n",
      "epochs: 16/30.....  training_step: 37950....  training_loss: 1.147049.......  1.9717 sec/batch \n",
      "epochs: 16/30.....  training_step: 38000....  training_loss: 1.156924.......  1.9582 sec/batch \n",
      "epochs: 16/30.....  training_step: 38050....  training_loss: 1.094841.......  1.9615 sec/batch \n",
      "epochs: 16/30.....  training_step: 38100....  training_loss: 1.098149.......  1.9570 sec/batch \n",
      "epochs: 16/30.....  training_step: 38150....  training_loss: 1.098179.......  1.9608 sec/batch \n",
      "epochs: 16/30.....  training_step: 38200....  training_loss: 1.148137.......  5.8183 sec/batch \n",
      "epochs: 16/30.....  training_step: 38250....  training_loss: 1.062118.......  1.9121 sec/batch \n",
      "epochs: 16/30.....  training_step: 38300....  training_loss: 1.160793.......  1.9629 sec/batch \n",
      "epochs: 16/30.....  training_step: 38350....  training_loss: 1.062979.......  1.8654 sec/batch \n",
      "epochs: 16/30.....  training_step: 38400....  training_loss: 1.139346.......  1.8809 sec/batch \n",
      "epochs: 16/30.....  training_step: 38450....  training_loss: 1.007230.......  1.8832 sec/batch \n",
      "epochs: 16/30.....  training_step: 38500....  training_loss: 1.122015.......  1.8762 sec/batch \n",
      "epochs: 16/30.....  training_step: 38550....  training_loss: 1.058608.......  1.8928 sec/batch \n",
      "epochs: 16/30.....  training_step: 38600....  training_loss: 1.118855.......  1.8688 sec/batch \n",
      "3776321\n",
      "epochs: 17/30.....  training_step: 38650....  training_loss: 1.164982.......  1.8749 sec/batch \n",
      "epochs: 17/30.....  training_step: 38700....  training_loss: 1.112260.......  1.9030 sec/batch \n",
      "epochs: 17/30.....  training_step: 38750....  training_loss: 1.158298.......  1.9331 sec/batch \n",
      "epochs: 17/30.....  training_step: 38800....  training_loss: 1.074456.......  1.9187 sec/batch \n",
      "epochs: 17/30.....  training_step: 38850....  training_loss: 1.171871.......  1.9395 sec/batch \n",
      "epochs: 17/30.....  training_step: 38900....  training_loss: 1.099662.......  1.9221 sec/batch \n",
      "epochs: 17/30.....  training_step: 38950....  training_loss: 1.104665.......  1.9195 sec/batch \n",
      "epochs: 17/30.....  training_step: 39000....  training_loss: 1.109795.......  1.9320 sec/batch \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 17/30.....  training_step: 39050....  training_loss: 1.110855.......  1.9374 sec/batch \n",
      "epochs: 17/30.....  training_step: 39100....  training_loss: 1.131273.......  1.9313 sec/batch \n",
      "epochs: 17/30.....  training_step: 39150....  training_loss: 1.110230.......  2.1413 sec/batch \n",
      "epochs: 17/30.....  training_step: 39200....  training_loss: 1.108661.......  2.4998 sec/batch \n",
      "epochs: 17/30.....  training_step: 39250....  training_loss: 1.079216.......  2.5588 sec/batch \n",
      "epochs: 17/30.....  training_step: 39300....  training_loss: 1.046615.......  1.9641 sec/batch \n",
      "epochs: 17/30.....  training_step: 39350....  training_loss: 1.073663.......  2.2858 sec/batch \n",
      "epochs: 17/30.....  training_step: 39400....  training_loss: 1.057717.......  2.5007 sec/batch \n",
      "epochs: 17/30.....  training_step: 39450....  training_loss: 1.086323.......  2.3885 sec/batch \n",
      "epochs: 17/30.....  training_step: 39500....  training_loss: 1.083881.......  2.3553 sec/batch \n",
      "epochs: 17/30.....  training_step: 39550....  training_loss: 1.092111.......  2.1659 sec/batch \n",
      "epochs: 17/30.....  training_step: 39600....  training_loss: 1.103176.......  2.2830 sec/batch \n",
      "epochs: 17/30.....  training_step: 39650....  training_loss: 1.074913.......  2.0859 sec/batch \n",
      "epochs: 17/30.....  training_step: 39700....  training_loss: 1.192453.......  2.1549 sec/batch \n",
      "epochs: 17/30.....  training_step: 39750....  training_loss: 1.123432.......  2.0926 sec/batch \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a6533302ce66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeed_state\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mtotal_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training phase\n",
    "\n",
    "#path_to_saved_model = './'\n",
    "print_every_n = 50\n",
    "epochs = 30\n",
    "save_every_steps = 500\n",
    "\n",
    "model = SherlockAI(len(create_set), lstmsize = lstmsize, learning_rate = learning_rate, \n",
    "                 batch_size = batch_size, num_steps = num_steps, num_layers = num_layers)\n",
    "\n",
    "counter= 0\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 100)\n",
    "initialized = tf.global_variables_initializer()\n",
    "\n",
    "with  tf.Session() as sess:\n",
    "    \n",
    "    #savede = tf.train.Saver()\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #restored = restore_vars(saver, sess, path_to_saved_model)\n",
    "    \n",
    "    \n",
    "    sess.run(initialized)\n",
    "    \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            #print(shape)\n",
    "            #print(len(shape))\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                #print(dim)\n",
    "                variable_parameters *= dim.value\n",
    "        \n",
    "            #print(variable_parameters)\n",
    "            total_parameters += variable_parameters\n",
    "        print(total_parameters)\n",
    "        \n",
    "        feed_state = sess.run(model.initial_state)\n",
    "    \n",
    "        \n",
    "        for x, y in get_batches(encoded_book, batch_size, num_steps):\n",
    "        \n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            \n",
    "            feed = {model.inputs: x, model.targets : y, model.keep_prob: keep_prob, model.initial_state: feed_state}\n",
    "            \n",
    "            batch_loss, feed_state, _ = sess.run([model.loss, model.final_state, model.optimizer], feed_dict = feed)\n",
    "            \n",
    "            total_parameters = 0\n",
    "            \n",
    "            \n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                \n",
    "                print('epochs: {}/{}..... '.format(i + 1, epochs),\n",
    "                     'training_step: {}.... '.format(counter),\n",
    "                      'training_loss: {:3f}....... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch '.format(end-start))\n",
    "                      \n",
    "               \n",
    "            if (counter % save_every_steps == 0):\n",
    "                \n",
    "                #saver.save(sess, 'checkpoints /{}___{}____{}.ckpt'.format(i +1 , counter, lstmsize))\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_h{}.ckpt\".format(counter, lstmsize, 5))\n",
    "                      \n",
    "                \n",
    "                      \n",
    "    #saver.save(sess, 'checkpoints /{}___{}____{}//final.ckpt'.format(i + 1, counter, lstmsize))\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}_h{}.ckpt\".format(counter, lstmsize, 5))\n",
    "                      \n",
    "    #samp = sample(checkpoint, 1000, lstmsize, len(create_set), prime=\"The\")\n",
    "    #print(samp)        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=2):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = dict(enumerate(create_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstmsize, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = SherlockAI(len(create_set), lstmsize=lstmsize, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        feed_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = dict_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(create_set))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: feed_state}\n",
    "            preds, feed_state = sess.run([model.predictions, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(create_set))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 500, lstmsize, len(create_set), prime=\"Sherlock\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
